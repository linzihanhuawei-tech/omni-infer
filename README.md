Omni_Infer is a suite of inference accelerators designed for the Ascend NPU platform, offering native support and an expanding feature set, including:

1. Enterprise-grade support for large-scale, disaggregated PD deployments
2. Request-level load balancing for prefill and decode, optimizing total throughput
3. Large-scale MoE expert deployment
4. MoE expert load balancing with layer-wise, uneven redundancy support
5. Attention optimizations for LLM, MLLM, and MoE models
