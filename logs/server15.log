INFO 10-24 15:28:36 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 10-24 15:28:36 [importing.py:28] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.
INFO 10-24 15:28:36 [__init__.py:31] Available plugins for group vllm.platform_plugins:
INFO 10-24 15:28:36 [__init__.py:33] - npu -> omni.adaptors.vllm.platform:register
INFO 10-24 15:28:36 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 10-24 15:28:36 [__init__.py:234] Platform plugin npu is activated
WARNING 10-24 15:28:37 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 10-24 15:28:39 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 10-24 15:28:39 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 10-24 15:28:39 [__init__.py:33] - kv_connectors -> omni.accelerators.pd:register
INFO 10-24 15:28:39 [__init__.py:33] - npu_optimized_models -> omni.models:register_model
INFO 10-24 15:28:39 [__init__.py:33] - reasoning -> omni.adaptors.vllm.reasoning:register_reasoning
INFO 10-24 15:28:39 [__init__.py:33] - tool_parsers -> omni.adaptors.vllm.entrypoints.openai.tool_parsers:register_tool
INFO 10-24 15:28:39 [__init__.py:33] - ascend_lmcache -> ascend_lmcache:register_ascend_lmcache
INFO 10-24 15:28:39 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_v3:DeepseekV3ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_mtp:DeepseekV3MTP.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Qwen2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2:Qwen2ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen3:Qwen3ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen3_moe:Qwen3MoeForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture LlamaForCausalLM is already registered, and will be overwritten by the new model class omni.models.llama.llama:LlamaForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2_5_vl:Qwen2_5_VLForConditionalGeneration.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2_vl:Qwen2VLForConditionalGeneration.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture InternLM2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.internvl.internlm2:InternLM2ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture InternVLChatModel is already registered, and will be overwritten by the new model class omni.models.internvl.internvl:InternVLChatModel.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Gemma3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.gemma.gemma3:Gemma3ForCausalLM.
WARNING 10-24 15:28:39 [registry.py:397] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.gemma.gemma3_mm:Gemma3ForConditionalGeneration.
/usr/local/lib/python3.11/site-packages/lmcache/usage_context.py:26: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
INFO 10-24 15:28:40 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.
++++++++++++++++++++++++patch_vllm_distributed++++++++++++++++++++++++++
+++++++++++++++++++++++patch_rope+++++++++++++++++++++++++++
++++++++++++++++++++++patch_sampler++++++++++++++++++++++++++++
+++++++++++++++++++++++patch_compilation+++++++++++++++++++++++++++
++++++++++++++++++++++patch_pangu++++++++++++++++++++++++++++
INFO 10-24 15:28:41 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 10-24 15:28:41 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 10-24 15:28:41 [api_server.py:1370] vLLM API server version 0.9.0
INFO 10-24 15:28:42 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 10-24 15:28:42 [cli_args.py:300] non-default args: {'port': 9179, 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 128, 'served_model_name': ['deepseek'], 'data_parallel_size_local': 1, 'data_parallel_address': '7.150.12.42', 'data_parallel_rpc_port': 7164, 'enable_expert_parallel': True, 'block_size': 128, 'gpu_memory_utilization': 0.8, 'enable_prefix_caching': False, 'max_num_seqs': 4, 'enable_chunked_prefill': False, 'kv_transfer_config': KVTransferConfig(kv_connector='AscendHcclConnectorV1', engine_id=1, kv_buffer_device='npu', kv_buffer_size=1000000000.0, kv_role='kv_consumer', kv_rank=1, kv_parallel_size=2, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None), 'disable_log_requests': True}
INFO 10-24 15:28:42 [config.py:3131] Downcasting torch.float32 to torch.bfloat16.
INFO 10-24 15:28:52 [config.py:793] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
WARNING 10-24 15:28:52 [arg_utils.py:1596] Detected VLLM_USE_V1=1 with npu. Usage should be considered experimental. Please report any issues on Github.
INFO 10-24 15:28:52 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 10-24 15:28:52 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 10-24 15:28:52 [config.py:2155] max_num_batched_tokens (2048) exceeds max_num_seqs* max_model_len (512). This may lead to unexpected behavior.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:53 [core.py:561] Waiting for init message from front-end.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:55 [core.py:69] Initializing a V1 LLM engine (v0.9.0) with config: model='/data/models/LongCat-Flash', speculative_config=None, tokenizer='/data/models/LongCat-Flash', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["all"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING 10-24 15:28:55 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING 10-24 15:28:56 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <omni.adaptors.vllm.worker.npu_worker.NPUWorker object at 0xfffcd50ddf90>
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] hardware_platform loads from vllm config: A3
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] is_pd_disaggregation loads from vllm config: True
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] is_prefill_node loads from vllm config: False
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] prefill_nodes_num loads from vllm config: 1
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] decode_nodes_num loads from vllm config: 1
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] enable_chunked_prefill loads from vllm config: True
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] enable_omni_placement loads from vllm config: False
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] decode_gear_list loads from vllm config: [4]
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:312] enable_graph_mode loads from vllm config: False
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:265] The task about longcat-flash_bf16_A3 load configuration file from /data/00943438/run_dir/omniinfer/omni/models/configs/longcat_decode_bf16_1p1d.json
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING 10-24 15:28:57 [loader.py:299] [WARNING] Eager mode disables all these optimization configurations by default.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [loader.py:31] Task config updated via 'update_task_config'
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:57 [parallel_state.py:933] Adjusting world_size=64 rank=15 distributed_init_method=tcp://7.150.12.42:7165 for DP
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:58 [parallel_state.py:1064] rank 15 in world size 64 is assigned as DP rank 15, PP rank 0, TP rank 0, EP rank 15
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:58 [factory.py:73] Creating v1 connector with name: AscendHcclConnectorV1
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:58 [llmdatadist_connector_v1.py:114] Set kv_parallel_size to 1 when use deepseek mla model.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:28:58 [npu_model_runner.py:1024] Starting to load model /data/models/LongCat-Flash...
[rank15]:[W1024 15:28:59.227123626 compiler_depend.ts:3003] Warning: The indexFromRank 0is not equal indexFromCurDevice 15 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:08 [default_loader.py:280] Loading weights took 8.83 seconds
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:09 [npu_model_runner.py:1036] Loading model weights took 30.3017 GB
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:16 [npu_worker.py:302] Available memory: 15701542502.400002, total memory: 65796046848
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:16 [kv_cache_utils.py:645] GPU KV cache size: 243,328 tokens
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:16 [kv_cache_utils.py:648] Maximum concurrency for 128 tokens per request: 1901.00x
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR:omni.accelerators.pd.llmdatadist_manager: ***** registered_kv_caches num:2
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:Current cluster id is 47, create link:{15: '7.150.8.102-15-7.150.12.42-47-p15-d15'}
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m WARNING:omni.accelerators.pd.llmdatadist_manager:rank:15 check link status
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:35 [llmdatadist_connector_v1.py:644]  ***** Using single thread to pull kv.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:35 [core.py:213] init engine (profile, create kv cache, warmup model) took 25.88 seconds
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:35 [factory.py:73] Creating v1 connector with name: AscendHcclConnectorV1
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:35 [llmdatadist_connector_v1.py:114] Set kv_parallel_size to 1 when use deepseek mla model.
[1;36m(EngineCore_15 pid=54250)[0;0m INFO 10-24 15:29:35 [scheduler.py:171]  combine_block_num is 1
INFO 10-24 15:29:35 [loggers.py:137] vllm cache_config_info with initialization after num_gpu_blocks is: 1901
INFO 10-24 15:29:37 [api_server.py:1417] Starting vLLM API server on http://0.0.0.0:9179
INFO 10-24 15:29:37 [launcher.py:28] Available routes are:
INFO 10-24 15:29:37 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 10-24 15:29:37 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 10-24 15:29:37 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 10-24 15:29:37 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 10-24 15:29:37 [launcher.py:36] Route: /health, Methods: GET
INFO 10-24 15:29:37 [launcher.py:36] Route: /load, Methods: GET
INFO 10-24 15:29:37 [launcher.py:36] Route: /ping, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /ping, Methods: GET
INFO 10-24 15:29:37 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 10-24 15:29:37 [launcher.py:36] Route: /version, Methods: GET
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /pooling, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /classify, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /score, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /rerank, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /invocations, Methods: POST
INFO 10-24 15:29:37 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [49920]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] EngineCore encountered a fatal error.
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] Traceback (most recent call last):
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 616, in run_engine_core
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     engine_core.run_busy_loop()
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 946, in run_busy_loop
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     self.execute_dummy_batch()
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 427, in execute_dummy_batch
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     self.model_executor.collective_rpc("execute_dummy_batch")
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return func(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 385, in execute_dummy_batch
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     self.model_runner._dummy_run(1)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return func(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 984, in _dummy_run
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     forward_results = self.model(
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                       ^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 93, in __call__
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return self.call_dispatcher(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 64, in call_dispatcher
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     hidden_states, residual = layer(positions,
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                               ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 199, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     hidden_states = self.self_attn[1](
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/layers/attention/deepseek_mla.py", line 575, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     output = self._forward_decode(positions, hidden_states, kv_cache, attn_metadata)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/layers/attention/deepseek_mla.py", line 1142, in _forward_decode
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     output, _ = self.o_proj.forward(attn_output, bsz, q_len, self.num_local_heads, self.v_head_dim)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/omni/layers/linear.py", line 418, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     output_parallel = self.quant_method.apply(self,
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/model_executor/layers/linear.py", line 202, in apply
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]     return dispatch_unquantized_gemm()(x, layer.weight, bias)
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] RuntimeError: RunOpApiV2:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:250 NPU function error: c10_npu::acl::AclrtSynchronizeStreamWithTimeout(stream), error code is 507034
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] [ERROR] 2025-10-24-15:33:35 (PID:54250, Device:15, RankID:-1) ERR00100 PTA call acl api failed
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] [Error]: Vector core execution timed out. 
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]         Rectify the fault based on the error information in the ascend log.
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] EZ9999: Inner Error!
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] EZ9999[PID: 54250] 2025-10-24-15:33:35.034.672 (EZ9999):  The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 2, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d708, vec error info: 0x1e00000090, mte error info: 0x58060000d9, ifu error info: 0x5782ecb221e40, ccu error info: 0xdc20c0393b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]         TraceBack (most recent call last):
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x60000d9, fixp_error1 info: 0x58, fsmId:1, tslot:0, thread:0, ctxid:0, blk:0, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 3, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d700, vec error info: 0x1e00000090, mte error info: 0xbc3f9b2825, ifu error info: 0x3fb33dc035000, ccu error info: 0x480423b73b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0xf9b2825, fixp_error1 info: 0xbc, fsmId:1, tslot:0, thread:0, ctxid:0, blk:1, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 4, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d700, vec error info: 0x1e00000090, mte error info: 0x8df1bff833, ifu error info: 0x8be38ca66e00, ccu error info: 0x22013c463b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x1bff833, fixp_error1 info: 0x8d, fsmId:1, tslot:0, thread:0, ctxid:0, blk:2, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 5, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d708, vec error info: 0x1e00000090, mte error info: 0x58060000d9, ifu error info: 0x460080b180c00, ccu error info: 0xa92580a769f46190, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x60000d9, fixp_error1 info: 0x58, fsmId:1, tslot:0, thread:0, ctxid:0, blk:3, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        Kernel task happen error, retCode=0x30, [vector core timeout].[FUNC:PreCheckTaskErr][FILE:davinci_kernel_task.cc][LINE:1555]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        AIV Kernel happen error, retCode=0x30.[FUNC:GetError][FILE:stream.cc][LINE:1184]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        rtStreamSynchronizeWithTimeout execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625]        synchronize stream with timeout failed, runtime result = 507034[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:162]
[1;36m(EngineCore_15 pid=54250)[0;0m ERROR 10-24 15:33:35 [core.py:625] 
ERROR 10-24 15:33:35 [async_llm.py:432] AsyncLLM output_handler failed.
ERROR 10-24 15:33:35 [async_llm.py:432] Traceback (most recent call last):
ERROR 10-24 15:33:35 [async_llm.py:432]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/async_llm.py", line 391, in output_handler
ERROR 10-24 15:33:35 [async_llm.py:432]     outputs = await engine_core.get_output_async()
ERROR 10-24 15:33:35 [async_llm.py:432]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 10-24 15:33:35 [async_llm.py:432]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 855, in get_output_async
ERROR 10-24 15:33:35 [async_llm.py:432]     raise self._format_exception(outputs) from None
ERROR 10-24 15:33:35 [async_llm.py:432] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[1;36m(EngineCore_15 pid=54250)[0;0m Process EngineCore_15:
[1;36m(EngineCore_15 pid=54250)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_15 pid=54250)[0;0m     self.run()
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_15 pid=54250)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 627, in run_engine_core
[1;36m(EngineCore_15 pid=54250)[0;0m     raise e
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 616, in run_engine_core
[1;36m(EngineCore_15 pid=54250)[0;0m     engine_core.run_busy_loop()
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 946, in run_busy_loop
[1;36m(EngineCore_15 pid=54250)[0;0m     self.execute_dummy_batch()
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 427, in execute_dummy_batch
[1;36m(EngineCore_15 pid=54250)[0;0m     self.model_executor.collective_rpc("execute_dummy_batch")
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
[1;36m(EngineCore_15 pid=54250)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method
[1;36m(EngineCore_15 pid=54250)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 385, in execute_dummy_batch
[1;36m(EngineCore_15 pid=54250)[0;0m     self.model_runner._dummy_run(1)
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_15 pid=54250)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 984, in _dummy_run
[1;36m(EngineCore_15 pid=54250)[0;0m     forward_results = self.model(
[1;36m(EngineCore_15 pid=54250)[0;0m                       ^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 93, in __call__
[1;36m(EngineCore_15 pid=54250)[0;0m     return self.call_dispatcher(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 64, in call_dispatcher
[1;36m(EngineCore_15 pid=54250)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(EngineCore_15 pid=54250)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m     hidden_states, residual = layer(positions,
[1;36m(EngineCore_15 pid=54250)[0;0m                               ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 199, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m     hidden_states = self.self_attn[1](
[1;36m(EngineCore_15 pid=54250)[0;0m                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(EngineCore_15 pid=54250)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/layers/attention/deepseek_mla.py", line 575, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m     output = self._forward_decode(positions, hidden_states, kv_cache, attn_metadata)
[1;36m(EngineCore_15 pid=54250)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/layers/attention/deepseek_mla.py", line 1142, in _forward_decode
[1;36m(EngineCore_15 pid=54250)[0;0m     output, _ = self.o_proj.forward(attn_output, bsz, q_len, self.num_local_heads, self.v_head_dim)
[1;36m(EngineCore_15 pid=54250)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/omni/layers/linear.py", line 418, in forward
[1;36m(EngineCore_15 pid=54250)[0;0m     output_parallel = self.quant_method.apply(self,
[1;36m(EngineCore_15 pid=54250)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/model_executor/layers/linear.py", line 202, in apply
[1;36m(EngineCore_15 pid=54250)[0;0m     return dispatch_unquantized_gemm()(x, layer.weight, bias)
[1;36m(EngineCore_15 pid=54250)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_15 pid=54250)[0;0m RuntimeError: RunOpApiV2:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:250 NPU function error: c10_npu::acl::AclrtSynchronizeStreamWithTimeout(stream), error code is 507034
[1;36m(EngineCore_15 pid=54250)[0;0m [ERROR] 2025-10-24-15:33:35 (PID:54250, Device:15, RankID:-1) ERR00100 PTA call acl api failed
[1;36m(EngineCore_15 pid=54250)[0;0m [Error]: Vector core execution timed out. 
[1;36m(EngineCore_15 pid=54250)[0;0m         Rectify the fault based on the error information in the ascend log.
[1;36m(EngineCore_15 pid=54250)[0;0m EZ9999: Inner Error!
[1;36m(EngineCore_15 pid=54250)[0;0m EZ9999[PID: 54250] 2025-10-24-15:33:35.034.672 (EZ9999):  The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 2, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d708, vec error info: 0x1e00000090, mte error info: 0x58060000d9, ifu error info: 0x5782ecb221e40, ccu error info: 0xdc20c0393b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m         TraceBack (most recent call last):
[1;36m(EngineCore_15 pid=54250)[0;0m        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x60000d9, fixp_error1 info: 0x58, fsmId:1, tslot:0, thread:0, ctxid:0, blk:0, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 3, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d700, vec error info: 0x1e00000090, mte error info: 0xbc3f9b2825, ifu error info: 0x3fb33dc035000, ccu error info: 0x480423b73b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0xf9b2825, fixp_error1 info: 0xbc, fsmId:1, tslot:0, thread:0, ctxid:0, blk:1, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 4, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d700, vec error info: 0x1e00000090, mte error info: 0x8df1bff833, ifu error info: 0x8be38ca66e00, ccu error info: 0x22013c463b000053, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x1bff833, fixp_error1 info: 0x8d, fsmId:1, tslot:0, thread:0, ctxid:0, blk:2, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m        The error from device(chipId:7, dieId:1), serial number is 9, there is an exception of aivec error, core id is 5, error code = 0, dump info: pc start: 0x12c956d10ea8, current: 0x12c956d1d708, vec error info: 0x1e00000090, mte error info: 0x58060000d9, ifu error info: 0x460080b180c00, ccu error info: 0xa92580a769f46190, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c1002f2000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:332]
[1;36m(EngineCore_15 pid=54250)[0;0m        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x60000d9, fixp_error1 info: 0x58, fsmId:1, tslot:0, thread:0, ctxid:0, blk:3, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:352]
[1;36m(EngineCore_15 pid=54250)[0;0m        Kernel task happen error, retCode=0x30, [vector core timeout].[FUNC:PreCheckTaskErr][FILE:davinci_kernel_task.cc][LINE:1555]
[1;36m(EngineCore_15 pid=54250)[0;0m        AIV Kernel happen error, retCode=0x30.[FUNC:GetError][FILE:stream.cc][LINE:1184]
[1;36m(EngineCore_15 pid=54250)[0;0m        rtStreamSynchronizeWithTimeout execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
[1;36m(EngineCore_15 pid=54250)[0;0m        synchronize stream with timeout failed, runtime result = 507034[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:162]
[1;36m(EngineCore_15 pid=54250)[0;0m 
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [49920]
