{\rtf1\ansi\ansicpg936\deff0\deflang1033\deflangfe2052{\fonttbl{\f0\fmodern Consolas;}{\f1\fnil\fcharset129 Courier New;}}
{\colortbl ;\red20\green20\blue20;\red142\green142\blue142;\red56\green136\blue159;\red9\green118\blue72;}
\viewkind4\uc1\pard\lang2052\f0\fs20 [root@devserver-hps-a0f74d4d-g00615224-00030 p0]# tail -f server_0.log\cf1\highlight2 
\par \cf0\highlight0     self.engine_core = core_client_class(\cf1\highlight2 
\par \cf0\highlight0                        ^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 781, in __init__\cf1\highlight2 
\par \cf0\highlight0     super().__init__(\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 425, in __init__\cf1\highlight2 
\par \cf0\highlight0     self._wait_for_engine_startup(output_address, parallel_config)\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 494, in _wait_for_engine_startup\cf1\highlight2 
\par \cf0\highlight0     raise RuntimeError("Engine core initialization failed. "\cf1\highlight2 
\par \cf0\highlight0 RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): \{'EngineCore_0': 1\}\cf1\highlight2 
\par \cf0\highlight0 [ERROR] 2025-10-24-17:55:13 (PID:209044, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 [root@devserver-hps-a0f74d4d-g00615224-00030 p0]# tail -f server_0.log\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:20 [cli_args.py:300] non-default args: \{'port': 9000, 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 128, 'enforce_eager': True, 'served_model_name': ['deepseek'], 'distributed_executor_backend': 'ray', 'tensor_parallel_size': 32, 'data_parallel_size_local': 1, 'data_parallel_address': '7.150.8.102', 'data_parallel_rpc_port': 7000, 'enable_expert_parallel': True, 'block_size': 128, 'gpu_memory_utilization': 0.88, 'enable_prefix_caching': False, 'max_num_batched_tokens': 128, 'max_num_seqs': 4, 'enable_chunked_prefill': False, 'kv_transfer_config': KVTransferConfig(kv_connector='AscendHcclConnectorV1', engine_id=0, kv_buffer_device='npu', kv_buffer_size=1000000000.0, kv_role='kv_producer', kv_rank=0, kv_parallel_size=2, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config=\{\}, kv_connector_module_path=None), 'disable_log_requests': True\}\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:20 [config.py:3131] Downcasting torch.float32 to torch.bfloat16.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:27 [config.py:793] This model supports multiple tasks: \{'embed', 'score', 'reward', 'generate', 'classify'\}. Defaulting to 'generate'.\cf1\highlight2 
\par \cf0\highlight0 WARNING 10-24 18:04:27 [arg_utils.py:1596] Detected VLLM_USE_V1=1 with npu. Usage should be considered experimental. Please report any issues on Github.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:27 [config.py:1909] Disabled the custom all-reduce kernel because it is not supported on current platform.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:27 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=128.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:28 [core.py:561] Waiting for init message from front-end.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:28 [core.py:69] Initializing a V1 LLM engine (v0.9.0) with config: model='/data/models/LongCat-Flash', speculative_config=None, tokenizer='/data/models/LongCat-Flash', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=\{\}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=32, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config=\{"custom_ops": ["all"], "compile_sizes": [], "cudagraph_capture_sizes": [], "max_capture_size": 0\}\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:04:28,223 INFO worker.py:1747 -- Connecting to existing Ray cluster at address: 7.150.8.102:6379...\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:04:28,232 INFO worker.py:1927 -- Connected to Ray cluster.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:33 [ray_utils.py:333] No current placement group found. Creating a new placement group.\cf1\highlight2 
\par \cf0\highlight0 WARNING 10-24 18:04:33 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\cf1\highlight2 
\par \cf0\highlight0 WARNING 10-24 18:04:33 [ray_utils.py:197] tensor_parallel_size=32 is bigger than a reserved number of NPUs (16 NPUs) in a node 06d1228c31e0752be94163a518d6d4044f33819eb6e42be7baf84cf0. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 32 GPUs available at each node.\cf1\highlight2 
\par \cf0\highlight0 WARNING 10-24 18:04:33 [ray_utils.py:197] tensor_parallel_size=32 is bigger than a reserved number of NPUs (16 NPUs) in a node 5c86fe9853899f2902d57d21d081280a0a2a65d08c5b9e1b12e0350a. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 32 GPUs available at each node.\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:33 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\cf1\highlight2 
\par \cf0\highlight0 WARNING 10-24 18:04:33 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  /usr/local/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0    import pkg_resources\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  INFO 10-24 18:04:36 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  WARNING 10-24 18:04:36 [importing.py:28] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  INFO 10-24 18:04:36 [__init__.py:31] Available plugins for group vllm.platform_plugins:\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  INFO 10-24 18:04:36 [__init__.py:33] - npu -> omni.adaptors.vllm.platform:register\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  INFO 10-24 18:04:36 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  INFO 10-24 18:04:36 [__init__.py:234] Platform plugin npu is activated\cf1\highlight2 
\par \cf3\highlight0 (pid=235633)\cf0  WARNING 10-24 18:04:38 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:39 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\cf1\highlight2 
\par \cf3\highlight0 (pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:37 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.\cf4  [repeated 31x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\cf1\highlight2 
\par \cf3\highlight0 (pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:37 [importing.py:28] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:41 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:41 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_LOGGING_LEVEL', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1', 'VLLM_DP_RANK', 'VLLM_DP_RANK_LOCAL', 'VLLM_DP_SIZE', 'VLLM_DP_MASTER_IP', 'VLLM_DP_MASTER_PORT']\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:04:41 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:31] Available plugins for group vllm.general_plugins:\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - kv_connectors -> omni.accelerators.pd:register\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - npu_optimized_models -> omni.models:register_model\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - reasoning -> omni.adaptors.vllm.reasoning:register_reasoning\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - tool_parsers -> omni.adaptors.vllm.entrypoints.openai.tool_parsers:register_tool\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_v2:CustomDeepseekV2ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_v3:DeepseekV3ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_mtp:DeepseekV3MTP.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Qwen2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2:Qwen2ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen3:Qwen3ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class omni.models.qwen.qwen3_moe:Qwen3MoeForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture LlamaForCausalLM is already registered, and will be overwritten by the new model class omni.models.llama.llama:LlamaForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2_5_vl:Qwen2_5_VLForConditionalGeneration.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.qwen.qwen2_vl:Qwen2VLForConditionalGeneration.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture InternLM2ForCausalLM is already registered, and will be overwritten by the new model class omni.models.internvl.internlm2:InternLM2ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture InternVLChatModel is already registered, and will be overwritten by the new model class omni.models.internvl.internvl:InternVLChatModel.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Gemma3ForCausalLM is already registered, and will be overwritten by the new model class omni.models.gemma.gemma3:Gemma3ForCausalLM.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.gemma.gemma3_mm:Gemma3ForConditionalGeneration.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  ++++++++++++++++++++++++patch_vllm_distributed++++++++++++++++++++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  +++++++++++++++++++++++patch_rope+++++++++++++++++++++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  ++++++++++++++++++++++patch_sampler++++++++++++++++++++++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  +++++++++++++++++++++++patch_compilation+++++++++++++++++++++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  ++++++++++++++++++++++patch_pangu++++++++++++++++++++++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  +++++++ patch_enable_max_token_exclude_reasoning ++++++++\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:42 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <omni.adaptors.vllm.worker.npu_worker.NPUWorker object at 0xffcfbdfae450>\cf1\highlight2 
\par \cf3\highlight0 (pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:38 [__init__.py:31] Available plugins for group vllm.platform_plugins:\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:38 [__init__.py:33] - npu -> omni.adaptors.vllm.platform:register\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\cf4  [repeated 63x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:38 [__init__.py:234] Platform plugin npu is activated\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] hardware_platform loads from vllm config: A3\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] is_pd_disaggregation loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] is_prefill_node loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] prefill_nodes_num loads from vllm config: 1\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] decode_nodes_num loads from vllm config: 1\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] enable_chunked_prefill loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] enable_omni_placement loads from vllm config: False\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] decode_gear_list loads from vllm config: [4]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:312] enable_graph_mode loads from vllm config: False\cf1\highlight2 
\par \cf3\highlight0 (pid=83899, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:40 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:41 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:31] Available plugins for group vllm.general_plugins:\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - kv_connectors -> omni.accelerators.pd:register\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - npu_optimized_models -> omni.models:register_model\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - reasoning -> omni.adaptors.vllm.reasoning:register_reasoning\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - tool_parsers -> omni.adaptors.vllm.entrypoints.openai.tool_parsers:register_tool\cf4  [repeated 31x across cluster]\cf3 (RayWorkerWrapper pid=83896, ip=7.150.15.197)\cf0  [rank20]:[W1024 18:04:54.058612119 compiler_depend.ts:2977] Warning: The indexFromRank 0is not equal indexFromCurDevice 4 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83899, ip=7.150.15.197)\cf0  /usr/local/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83899, ip=7.150.15.197)\cf0    import pkg_resources\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:   0% Completed | 0/75 [00:00<?, ?it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:   3% Completed | 2/75 [00:00<00:04, 15.93it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:   5% Completed | 4/75 [00:00<00:06, 10.20it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:   8% Completed | 6/75 [00:00<00:08,  8.62it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:   9% Completed | 7/75 [00:00<00:08,  8.31it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  11% Completed | 8/75 [00:00<00:08,  8.24it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  12% Completed | 9/75 [00:01<00:08,  8.02it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  13% Completed | 10/75 [00:01<00:08,  7.83it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  15% Completed | 11/75 [00:01<00:08,  7.62it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  16% Completed | 12/75 [00:01<00:08,  7.35it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  19% Completed | 14/75 [00:01<00:06,  9.29it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  20% Completed | 15/75 [00:01<00:06,  8.72it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  21% Completed | 16/75 [00:01<00:07,  8.33it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  23% Completed | 17/75 [00:02<00:07,  7.87it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  24% Completed | 18/75 [00:02<00:07,  7.58it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  25% Completed | 19/75 [00:02<00:07,  7.48it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  27% Completed | 20/75 [00:02<00:07,  7.26it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  28% Completed | 21/75 [00:02<00:07,  7.37it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  29% Completed | 22/75 [00:02<00:07,  7.32it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  31% Completed | 23/75 [00:02<00:07,  7.14it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  32% Completed | 24/75 [00:02<00:06,  7.38it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  33% Completed | 25/75 [00:03<00:06,  7.31it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  35% Completed | 26/75 [00:03<00:06,  7.39it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  37% Completed | 28/75 [00:03<00:04,  9.53it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  39% Completed | 29/75 [00:03<00:05,  9.04it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  40% Completed | 30/75 [00:03<00:05,  8.47it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  41% Completed | 31/75 [00:03<00:05,  8.17it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  43% Completed | 32/75 [00:03<00:05,  7.90it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  44% Completed | 33/75 [00:04<00:05,  7.73it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  45% Completed | 34/75 [00:04<00:05,  7.82it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  47% Completed | 35/75 [00:04<00:05,  7.79it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  48% Completed | 36/75 [00:04<00:05,  7.66it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  49% Completed | 37/75 [00:04<00:05,  7.52it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  51% Completed | 38/75 [00:04<00:04,  7.68it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  52% Completed | 39/75 [00:04<00:04,  7.77it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  53% Completed | 40/75 [00:04<00:04,  7.68it/s]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83899, ip=7.150.15.197)\cf0  [rank25]:[W1024 18:04:54.164266627 compiler_depend.ts:2977] Warning: The indexFromRank 0is not equal indexFromCurDevice 9 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())\cf4  [repeated 29x across cluster]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  55% Completed | 41/75 [00:05<00:04,  7.52it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  56% Completed | 42/75 [00:05<00:04,  7.68it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  57% Completed | 43/75 [00:05<00:04,  7.68it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  59% Completed | 44/75 [00:05<00:03,  7.85it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  60% Completed | 45/75 [00:05<00:03,  7.64it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  61% Completed | 46/75 [00:05<00:03,  7.74it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  64% Completed | 48/75 [00:05<00:02,  9.76it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  65% Completed | 49/75 [00:06<00:02,  9.26it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  67% Completed | 50/75 [00:06<00:02,  8.81it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  68% Completed | 51/75 [00:06<00:02,  8.57it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  69% Completed | 52/75 [00:06<00:02,  8.41it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  71% Completed | 53/75 [00:06<00:02,  8.07it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  72% Completed | 54/75 [00:06<00:02,  7.79it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  73% Completed | 55/75 [00:06<00:02,  7.46it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  75% Completed | 56/75 [00:06<00:02,  7.30it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  76% Completed | 57/75 [00:07<00:02,  7.19it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  77% Completed | 58/75 [00:07<00:02,  7.35it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  79% Completed | 59/75 [00:07<00:02,  7.16it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  80% Completed | 60/75 [00:07<00:02,  6.89it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  81% Completed | 61/75 [00:07<00:02,  6.77it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  83% Completed | 62/75 [00:07<00:01,  6.72it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  84% Completed | 63/75 [00:08<00:01,  6.83it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  85% Completed | 64/75 [00:08<00:01,  6.68it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  87% Completed | 65/75 [00:08<00:01,  6.72it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  88% Completed | 66/75 [00:08<00:01,  6.68it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  89% Completed | 67/75 [00:08<00:01,  6.65it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  91% Completed | 68/75 [00:08<00:01,  6.74it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  92% Completed | 69/75 [00:08<00:00,  6.91it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  93% Completed | 70/75 [00:09<00:00,  7.11it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  95% Completed | 71/75 [00:09<00:00,  7.07it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  97% Completed | 73/75 [00:09<00:00,  9.34it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards:  99% Completed | 74/75 [00:09<00:00,  8.73it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards: 100% Completed | 75/75 [00:09<00:00,  8.12it/s]\cf1\highlight2 
\par \cf0\highlight0 Loading safetensors checkpoint shards: 100% Completed | 75/75 [00:09<00:00,  7.82it/s]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf1\highlight2 
\par 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:41 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class omni.models.gemma.gemma3_mm:Gemma3ForConditionalGeneration.\cf4  [repeated 310x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class omni.models.deepseek.deepseek_mtp:DeepseekV3MTP.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture LlamaForCausalLM is already registered, and will be overwritten by the new model class omni.models.llama.llama:LlamaForCausalLM.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:41 [registry.py:397] Model architecture InternVLChatModel is already registered, and will be overwritten by the new model class omni.models.internvl.internvl:InternVLChatModel.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:265] The task about longcat-flash_bf16_A3 load configuration file from /data/00943438/run_dir/omniinfer/omni/models/configs/longcat_prefill_bf16_1p1d.json\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  WARNING 10-24 18:04:47 [loader.py:299] [WARNING] Eager mode disables all these optimization configurations by default.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235631)\cf0  INFO 10-24 18:04:47 [loader.py:31] Task config updated via 'update_task_config'\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  ++++++++++++++++++++++++patch_vllm_distributed++++++++++++++++++++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  +++++++++++++++++++++++patch_rope+++++++++++++++++++++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  ++++++++++++++++++++++patch_sampler++++++++++++++++++++++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  +++++++++++++++++++++++patch_compilation+++++++++++++++++++++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  ++++++++++++++++++++++patch_pangu++++++++++++++++++++++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  +++++++ patch_enable_max_token_exclude_reasoning ++++++++\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:42 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <omni.adaptors.vllm.worker.npu_worker.NPUWorker object at 0xffcfd15c3290>\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] hardware_platform loads from vllm config: A3\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] is_pd_disaggregation loads from vllm config: True\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] is_prefill_node loads from vllm config: True\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] prefill_nodes_num loads from vllm config: 1\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] decode_nodes_num loads from vllm config: 1\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_chunked_prefill loads from vllm config: True\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_omni_placement loads from vllm config: False\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] decode_gear_list loads from vllm config: [4]\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_graph_mode loads from vllm config: False\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:265] The task about longcat-flash_bf16_A3 load configuration file from /data/00943438/run_dir/omniinfer/omni/models/configs/longcat_prefill_bf16_1p1d.json\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:52 [loader.py:299] [WARNING] Eager mode disables all these optimization configurations by default.\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83898, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:31] Task config updated via 'update_task_config'\cf4  [repeated 30x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  WARNING 10-24 18:04:53 [utils.py:578] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], buffer_handle=(15, 4194304, 6, 'psm_6275d706'), local_subscribe_addr='ipc:///tmp/9fafe96e-07b4-42ee-baaa-fdc53d64743c', remote_subscribe_addr='tcp://7.150.8.102:51795', remote_addr_ipv6=False)\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  INFO 10-24 18:04:53 [parallel_state.py:1064] rank 16 in world size 32 is assigned as DP rank 0, PP rank 0, TP rank 16, EP rank 16\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_063bf257'), local_subscribe_addr='ipc:///tmp/8bbd0248-4e68-44c3-ad8e-f671364d1637', remote_subscribe_addr=None, remote_addr_ipv6=False)\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [factory.py:73] Creating v1 connector with name: AscendHcclConnectorV1\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [llmdatadist_connector_v1.py:114] Set kv_parallel_size to 1 when use deepseek mla model.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [llmdatadist_connector_v1.py:287] ConnectWorker bind tcp://7.150.8.102:5568\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:04:53 [npu_model_runner.py:1024] Starting to load model /data/models/LongCat-Flash...\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:05:04 [default_loader.py:280] Loading weights took 9.64 seconds\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] hardware_platform loads from vllm config: A3\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] is_pd_disaggregation loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] is_prefill_node loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] prefill_nodes_num loads from vllm config: 1\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] decode_nodes_num loads from vllm config: 1\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_chunked_prefill loads from vllm config: True\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_omni_placement loads from vllm config: False\cf3 (RayWorkerWrapper pid=83890, ip=7.150.15.197)\cf0  [rank16]:[W1024 18:05:09.829325220 compiler_depend.ts:335] Warning: Cannot create tensor with interal format while allow_internel_format=False, tensor will be created with base format. (function operator())\cf1\highlight2 
\par 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] decode_gear_list loads from vllm config: [4]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:312] enable_graph_mode loads from vllm config: False\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:265] The task about longcat-flash_bf16_A3 load configuration file from /data/00943438/run_dir/omniinfer/omni/models/configs/longcat_prefill_bf16_1p1d.json\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  WARNING 10-24 18:04:52 [loader.py:299] [WARNING] Eager mode disables all these optimization configurations by default.\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=83903, ip=7.150.15.197)\cf0  INFO 10-24 18:04:52 [loader.py:31] Task config updated via 'update_task_config'\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235836)\cf0  INFO 10-24 18:04:53 [parallel_state.py:1064] rank 14 in world size 32 is assigned as DP rank 0, PP rank 0, TP rank 14, EP rank 14\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235639)\cf0  INFO 10-24 18:04:53 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_88b9a77d'), local_subscribe_addr='ipc:///tmp/c35d89d6-eeea-43fd-8cfd-d6cba7a4c6fa', remote_subscribe_addr=None, remote_addr_ipv6=False)\cf4  [repeated 9x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:53 [factory.py:73] Creating v1 connector with name: AscendHcclConnectorV1\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:53 [llmdatadist_connector_v1.py:114] Set kv_parallel_size to 1 when use deepseek mla model.\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=84219, ip=7.150.15.197)\cf0  INFO 10-24 18:04:53 [npu_model_runner.py:1024] Starting to load model /data/models/LongCat-Flash...\cf4  [repeated 31x across cluster]\cf1\highlight2 
\par \cf3\highlight0 (RayWorkerWrapper pid=235633)\cf0  INFO 10-24 18:05:04 [npu_model_runner.py:1036] Loading model weights took 41.6575 GB\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623] EngineCore failed to start.\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623] Traceback (most recent call last):\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 614, in run_engine_core\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     engine_core = EngineCoreProc(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 506, in __init__\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     super().__init__(vllm_config, executor_class, log_stats,\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 90, in __init__\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     self._initialize_kv_caches(vllm_config)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 183, in _initialize_kv_caches\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     available_gpu_memory = self.model_executor.determine_available_memory()\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/executor/abstract.py", line 75, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     output = self.collective_rpc("determine_available_memory")\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/executor_base.py", line 331, in collective_rpc\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self._run_workers(method, *args, **(kwargs or \{\}))\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/ray_distributed_executor.py", line 521, in _run_workers\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     ray_worker_outputs = ray.get(ray_worker_outputs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return fn(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/ray/_private/worker.py", line 2858, in get\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/ray/_private/worker.py", line 958, in get_objects\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     raise value.as_instanceof_cause()\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623] ray.exceptions.RayTaskError(AttributeError): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=235634, ip=7.150.8.102, actor_id=eb2eb377c3ed5bb7810af8ba01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffcfb7ae4790>)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     raise e\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623]                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 ERROR 10-24 18:05:12 [core.py:623] AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 Process EngineCore_0:\cf1\highlight2 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap\cf1\highlight2 
\par \cf0\highlight0     self.run()\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run\cf1\highlight2 
\par \cf0\highlight0     self._target(*self._args, **self._kwargs)\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 627, in run_engine_core\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 614, in run_engine_core\cf1\highlight2 
\par \cf0\highlight0     engine_core = EngineCoreProc(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 506, in __init__\cf1\highlight2 
\par \cf0\highlight0     super().__init__(vllm_config, executor_class, log_stats,\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 90, in __init__\cf1\highlight2 
\par \cf0\highlight0     self._initialize_kv_caches(vllm_config)\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core.py", line 183, in _initialize_kv_caches\cf1\highlight2 
\par \cf0\highlight0     available_gpu_memory = self.model_executor.determine_available_memory()\cf1\highlight2 
\par \cf0\highlight0                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/executor/abstract.py", line 75, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     output = self.collective_rpc("determine_available_memory")\cf1\highlight2 
\par \cf0\highlight0              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/executor_base.py", line 331, in collective_rpc\cf1\highlight2 
\par \cf0\highlight0     return self._run_workers(method, *args, **(kwargs or \{\}))\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/executor/ray_distributed_executor.py", line 521, in _run_workers\cf1\highlight2 
\par \cf0\highlight0     ray_worker_outputs = ray.get(ray_worker_outputs)\cf1\highlight2 
\par \cf0\highlight0                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper\cf1\highlight2 
\par \cf0\highlight0     return fn(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/ray/_private/worker.py", line 2858, in get\cf1\highlight2 
\par \cf0\highlight0     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\cf1\highlight2 
\par \cf0\highlight0                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/ray/_private/worker.py", line 958, in get_objects\cf1\highlight2 
\par \cf0\highlight0     raise value.as_instanceof_cause()\cf1\highlight2 
\par \cf0\highlight0 ray.exceptions.RayTaskError(AttributeError): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=235634, ip=7.150.8.102, actor_id=eb2eb377c3ed5bb7810af8ba01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffcfb7ae4790>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,082 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=84219, ip=7.150.15.197, actor_id=a8a5d0e8f52c5d36ad8f242501000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffcfcb766310>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,083 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=83902, ip=7.150.15.197, actor_id=1cfb54b8679df8f2bd146cac01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffffb80baad0>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,083 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=83898, ip=7.150.15.197, actor_id=54f5ea6c5bdb2e074efb3f5801000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffcfc8457450>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,083 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=83892, ip=7.150.15.197, actor_id=68915d0b19216854ab349b1601000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffff7b53b650>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,083 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=83896, ip=7.150.15.197, actor_id=be219fb4596741251c23998f01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffff8eac8650>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 INFO 10-24 18:05:12 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\cf1\highlight2 
\par \cf0\highlight0 2025-10-24 18:05:12,089 ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \cf3 ray::RayWorkerWrapper.execute_method()\cf0  (pid=235638, ip=7.150.8.102, actor_id=09aa43e00d458991a962e13c01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0xffff7b59a3d0>)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 621, in execute_method\cf1\highlight2 
\par \cf0\highlight0     raise e\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/worker/worker_base.py", line 612, in execute_method\cf1\highlight2 
\par \cf0\highlight0     return run_method(self, method, args, kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/utils.py", line 2605, in run_method\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 227, in determine_available_memory\cf1\highlight2 
\par \cf0\highlight0     cur_npu_kv_cache_bytes = self._compute_kv_cache_bytes()\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_worker.py", line 281, in _compute_kv_cache_bytes\cf1\highlight2 
\par \cf0\highlight0     self.model_runner.profile_run()\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 1016, in profile_run\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self._dummy_run(self.max_num_tokens)\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context\cf1\highlight2 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/worker/npu_model_runner.py", line 915, in _dummy_run\cf1\highlight2 
\par \cf0\highlight0     forward_results = self.model(\cf1\highlight2 
\par \cf0\highlight0                       ^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 94, in __call__\cf1\highlight2 
\par \cf0\highlight0     return self.call_dispatcher(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/adaptors/vllm/compilation/wrapper.py", line 65, in call_dispatcher\cf1\highlight2 
\par \cf0\highlight0     return self.forward(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 339, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states = self.model(input_ids, positions, kv_caches,\cf1\highlight2 
\par \cf0\highlight0                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 277, in forward\cf1\highlight2 
\par \cf0\highlight0     hidden_states, residual = layer(positions,\cf1\highlight2 
\par \cf0\highlight0                               ^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_flash.py", line 181, in forward\cf1\highlight2 
\par \cf0\highlight0     moe_hidden_states = self.mlp(moe_hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl\cf1\highlight2 
\par \cf0\highlight0     return self._call_impl(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl\cf1\highlight2 
\par \cf0\highlight0     return forward_call(*args, **kwargs)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 262, in forward\cf1\highlight2 
\par \cf0\highlight0     return self._forward_prefill_norm(hidden_states, attn_metadata)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 281, in _forward_prefill_norm\cf1\highlight2 
\par \cf0\highlight0     topk_weights, topk_ids = self.router.get_topk_indices(router_logits)\cf1\highlight2 
\par \cf0\highlight0                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/omni/models/longcat/longcat_moe.py", line 111, in get_topk_indices\cf1\highlight2 
\par \cf0\highlight0     scores_for_choice = scores.view(-1, n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\cf1\highlight2 
\par \cf0\highlight0                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0 AttributeError: 'NoneType' object has no attribute 'unsqueeze'\cf1\highlight2 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/bin/vllm", line 8, in <module>\cf1\highlight2 
\par \cf0\highlight0     sys.exit(main())\cf1\highlight2 
\par \cf0\highlight0              ^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/entrypoints/cli/main.py", line 56, in main\cf1\highlight2 
\par \cf0\highlight0     args.dispatch_function(args)\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/entrypoints/cli/serve.py", line 42, in cmd\cf1\highlight2 
\par \cf0\highlight0     uvloop.run(run_server(args))\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 105, in run\cf1\highlight2 
\par \cf0\highlight0     return runner.run(wrapper())\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run\cf1\highlight2 
\par \cf0\highlight0     return self._loop.run_until_complete(task)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 61, in wrapper\cf1\highlight2 
\par \cf0\highlight0     return await main\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/entrypoints/openai/api_server.py", line 1405, in run_server\cf1\highlight2 
\par \cf0\highlight0     async with build_async_engine_client(args) as engine_client:\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/contextlib.py", line 210, in __aenter__\cf1\highlight2 
\par \cf0\highlight0     return await anext(self.gen)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/entrypoints/openai/api_server.py", line 155, in build_async_engine_client\cf1\highlight2 
\par \cf0\highlight0     async with build_async_engine_client_from_engine_args(\cf1\highlight2 
\par \cf0\highlight0   File "/usr/local/lib/python3.11/contextlib.py", line 210, in __aenter__\cf1\highlight2 
\par \cf0\highlight0     return await anext(self.gen)\cf1\highlight2 
\par \cf0\highlight0            ^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args\cf1\highlight2 
\par \cf0\highlight0     async_llm = AsyncLLM.from_vllm_config(\cf1\highlight2 
\par \cf0\highlight0                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/async_llm.py", line 158, in from_vllm_config\cf1\highlight2 
\par \cf0\highlight0     return cls(\cf1\highlight2 
\par \cf0\highlight0            ^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/async_llm.py", line 117, in __init__\cf1\highlight2 
\par \cf0\highlight0     self.engine_core = core_client_class(\cf1\highlight2 
\par \cf0\highlight0                        ^^^^^^^^^^^^^^^^^^\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 781, in __init__\cf1\highlight2 
\par \cf0\highlight0     super().__init__(\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 425, in __init__\cf1\highlight2 
\par \cf0\highlight0     self._wait_for_engine_startup(output_address, parallel_config)\cf1\highlight2 
\par \cf0\highlight0   File "/data/00943438/run_dir/omniinfer/infer_engines/vllm/vllm/v1/engine/core_client.py", line 494, in _wait_for_engine_startup\cf1\highlight2 
\par \cf0\highlight0     raise RuntimeError("Engine core initialization failed. "\cf1\highlight2 
\par \cf0\highlight0 RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): \{'EngineCore_0': 1\}\cf1\highlight2 
\par \cf0\highlight0 [ERROR] 2025-10-24-18:05:12 (PID:235306, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception\cf1\highlight2 
\par \pard\cf0\highlight0\f1 
\par }
 