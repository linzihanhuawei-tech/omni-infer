Subject: [PATCH] operator change to improve profiler
---
Index: python/sglang/srt/layers/quantization/w8a8_int8.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/layers/quantization/w8a8_int8.py b/python/sglang/srt/layers/quantization/w8a8_int8.py
--- a/python/sglang/srt/layers/quantization/w8a8_int8.py	(revision 5c2f75c10f5ae5ab139f1f9746920a0a6a7938c3)
+++ b/python/sglang/srt/layers/quantization/w8a8_int8.py	(revision e461fbc02961cd3353ae9386f5a21d53a8382926)
@@ -320,6 +320,7 @@
 
     def __init__(self, quantization_config: W8A8Int8Config):
         self.quantization_config = quantization_config
+        self.enable_weight_nz = _is_npu
 
     def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
         if _is_cpu:
@@ -331,6 +332,10 @@
 
         layer.weight = Parameter(layer.weight.t(), requires_grad=False)
         layer.weight_scale = Parameter(layer.weight_scale.data, requires_grad=False)
+        if self.enable_weight_nz:
+            layer.weight.data = torch_npu.npu_format_cast(
+                layer.weight.data.contiguous(), 29
+            )
 
     def create_weights(
         self,
@@ -378,12 +383,27 @@
                 x.dtype,
                 True,  # is_vnni
             )
-
-        x_q, x_scale = per_token_quant_int8(x)
-
-        return int8_scaled_mm(
-            x_q, layer.weight, x_scale, layer.weight_scale, out_dtype=x.dtype, bias=bias
-        )
+        if _is_cpu:
+            x_q, x_scale = torch_npu.npu_dynamic_quant(x)
+            out = torch_npu.npu_quant_matmul(
+                x_q,
+                layer.weight,
+                layer.weight_scale.view(-1),
+                pretoken_scale=x_scale.view(-1),
+                bias=bias,
+                output_dtype=x.dtype,
+            )
+        else:
+            x_q, x_scale = per_token_quant_int8(x)
+            out = int8_scaled_mm(
+                x_q,
+                layer.weight,
+                x_scale,
+                layer.weight_scale,
+                out_dtype=x.dtype,
+                bias=bias
+            )
+        return out
 
 
 class W8A8Int8MoEMethod(FusedMoEMethodBase):
@@ -399,6 +419,7 @@
 
     def __init__(self, quant_config: W8A8Int8Config):
         self.quant_config = quant_config
+        self.enable_weight_nz = _is_npu
 
     def create_weights(
         self,
@@ -470,6 +491,15 @@
         layer.w2_weight_scale = Parameter(
             layer.w2_weight_scale.data, requires_grad=False
         )
+        if self.enable_weight_nz:
+            layer.w13_weight_nz = layer.w13_weight.npu()
+            layer.w2_weight = layer.w2_weight.npu()
+            layer.w13_weight.data = torch_npu.npu_format_cast(
+                layer.w13_weight.data, 29
+            )
+            layer.w2_weight.data = torch_npu.npu_format_cast(
+                layer.w2_weight.data, 29
+            )
 
     def apply(
         self,
Index: python/sglang/srt/managers/scheduler_profiler_mixin.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/managers/scheduler_profiler_mixin.py b/python/sglang/srt/managers/scheduler_profiler_mixin.py
--- a/python/sglang/srt/managers/scheduler_profiler_mixin.py	(revision 5c2f75c10f5ae5ab139f1f9746920a0a6a7938c3)
+++ b/python/sglang/srt/managers/scheduler_profiler_mixin.py	(revision e461fbc02961cd3353ae9386f5a21d53a8382926)
@@ -141,14 +141,6 @@
             self.rpd_profiler.start()
             self.rpd_profiler.rangePush("", "rpd profile range", "")
             self.profile_in_progress = True
-        elif torchprof_activities:
-            self.torch_profiler = torch.profiler.profile(
-                activities=torchprof_activities,
-                with_stack=with_stack if with_stack is not None else True,
-                record_shapes=record_shapes if record_shapes is not None else False,
-            )
-            self.torch_profiler.start()
-            self.profile_in_progress = True
         elif _is_npu:
             experimental_config = torch_npu.profiler._ExperimentalConfig(
                 profiler_level=torch_npu.profiler.ProfilerLevel.Level1,
@@ -168,7 +160,14 @@
             )
             self.torch_profiler.start()
             self.profile_in_progress = True
-
+        elif torchprof_activities:
+            self.torch_profiler = torch.profiler.profile(
+                activities=torchprof_activities,
+                with_stack=with_stack if with_stack is not None else True,
+                record_shapes=record_shapes if record_shapes is not None else False,
+            )
+            self.torch_profiler.start()
+            self.profile_in_progress = True
         if "MEM" in activities:
             torch.cuda.memory._record_memory_history(max_entries=100000)
             self.profile_in_progress = True
Index: python/sglang/srt/models/deepseek_v2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
--- a/python/sglang/srt/models/deepseek_v2.py	(revision 5c2f75c10f5ae5ab139f1f9746920a0a6a7938c3)
+++ b/python/sglang/srt/models/deepseek_v2.py	(revision e461fbc02961cd3353ae9386f5a21d53a8382926)
@@ -110,10 +110,12 @@
     is_non_idle_and_non_empty,
     log_info_on_rank0,
     use_intel_amx_backend,
+    is_npu,
 )
 
 _is_hip = is_hip()
 _is_cuda = is_cuda()
+_is_npu = is_npu()
 _is_fp8_fnuz = is_fp8_fnuz()
 _use_aiter = get_bool_env_var("SGLANG_USE_AITER") and _is_hip
 _is_cpu_amx_available = cpu_has_amx_support()
@@ -1334,6 +1336,20 @@
                 torch.bfloat16,
             )
             attn_bmm_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
+        elif _is_npu:
+            attn_bmm_output = torch.empty(
+                (self.num_local_heads, attn_output.shape[0], self.y_head_dim),
+                dtype=attn_output.dtype,
+                device=attn_output.device,
+            )
+            torch.bmm(
+                attn_output.transpose(0, 1),
+                self.w_vc,
+                out=attn_bmm_output,
+            )
+            attn_bmm_output = attn_bmm_output.transpose(0, 1).reshape(
+                -1, self.num_local_heads * self.v_head_dim
+            )
         else:
             attn_bmm_output = torch.empty(
                 (attn_output.shape[0], self.num_local_heads * self.v_head_dim),
