diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py
index 71fe73686..f22f113dc 100644
--- a/python/sglang/srt/distributed/parallel_state.py
+++ b/python/sglang/srt/distributed/parallel_state.py
@@ -581,6 +581,19 @@ class GroupCoordinator:
         torch.distributed.reduce_scatter(output, input_list, group=self.device_group)
         return output
 
+    def reduce_scatter_(
+        self,
+        input_: torch.Tensor,
+    ) -> None:
+        input_size = tuple(input_.size())
+        output = torch.empty(
+            (input_size[0] // self.world_size,) + input_size[1:],
+            dtype=input_.dtype,
+            device=input_.device,
+        )
+        torch.distributed.reduce_scatter_tensor(output, input_, group=self.device_group)
+        return output
+
     def _all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
         pynccl_comm = self.pynccl_comm
         if pynccl_comm is not None and not pynccl_comm.disabled:
@@ -590,6 +603,88 @@ class GroupCoordinator:
                 output, input, group=self.device_group
             )
 
+    def all_to_all(
+            self,
+            input_: torch.Tensor,
+            scatter_dim: int = 0,
+            gather_dim: int = -1,
+            scatter_sizes: Optional[List[int]] = None,
+            gather_sizes: Optional[List[int]] = None
+    ) -> torch.Tensor:
+        if self.world_size == 1:
+            return input_
+        scatter_dim = self._normalize_dim(scatter_dim, input_.dim())
+        gather_dim = self._normalize_dim(gather_dim, input_.dim())
+        self._validate_inputs(input_, scatter_dim, gather_sizes, scatter_sizes)
+        input_list = self._prepare_input_list(input_, scatter_dim, scatter_sizes)
+        output_list = self._prepare_output_list(input_list, gather_dim, gather_sizes)
+        torch.distributed.all_to_all(output_list, input_list, group=self.device_group)
+        return torch.cat(output_list, dim=gather_dim).contiguous()
+
+    def _normalize_dim(self, dim: int, tensor_dim: int) -> int:
+        if dim < -tensor_dim or dim >= tensor_dim:
+            raise ValueError(f"Dimension {dim} is out of bounds for tensor with {tensor_dim} dimensions")
+        return dim if dim >= 0 else dim + tensor_dim
+
+    def _validate_inputs(
+            self,
+            input_: torch.Tensor,
+            scatter_dim: int,
+            gather_sizes: Optional[List[int]],
+            scatter_sizes: Optional[List[int]]
+        ) -> None:
+        if scatter_sizes is not None and gather_sizes is not None:
+            if len(scatter_sizes) != self.world_size or len(gather_sizes) != self.world_size:
+                raise ValueError(
+                    f"scatter_sizes and gather_sizes must have length {self.world_size}, "
+                    f"got {len(scatter_sizes)} and {len(gather_sizes)}"
+                )
+            if sum(scatter_sizes) != input_.size(scatter_dim):
+                raise ValueError(
+                    f"Sum of scatter_sizes ({sum(scatter_sizes)}) does not match "
+                    f"input size at scatter_dim ({input_.size(scatter_dim)})"
+                )
+        elif scatter_sizes is not None or gather_sizes is not None:
+            raise ValueError("Both scatter_sizes and gather_sizes must be provided or neither")
+
+    def _prepare_input_list(
+            self,
+            input_: torch.Tensor,
+            scatter_dim: int,
+            scatter_sizes: Optional[List[int]]
+    ) -> List[torch.Tensor]:
+        if scatter_sizes is not None:
+            return [
+                t.contiguous() for t in torch.split(input_, scatter_sizes, scatter_dim)
+            ]
+        return [
+            t.contiguous()
+            for t in torch.tensor_split(input_, self.world_size, scatter_dim)
+        ]
+
+    def _prepare_output_list(
+        self,
+        input_list: List[torch.Tensor],
+        gather_dim: int,
+        gather_sizes: Optional[List[int]],
+    ) -> List[torch.Tensor]:
+        if gather_sizes is not None:
+            tensor_shape_base = input_list[self.rank].size()
+            output_list = []
+            for size in gather_sizes:
+                tensor_shape = list(tensor_shape_base)
+                tensor_shape[gather_dim] = size
+                output_list.append(
+                    torch.empty(
+                        tensor_shape,
+                        dtype=input_list[0].dtype,
+                        device=input_list[0].device,
+                    )
+                )
+        else:
+            output_list = [torch.empty_like(chunk) for chunk in input_list]
+        return output_list
+
     def all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
         if not supports_custom_op() or is_npu():
             self._all_gather_into_tensor(output, input)
@@ -1288,6 +1383,7 @@ def initialize_model_parallel(
     pipeline_model_parallel_size: int = 1,
     backend: Optional[str] = None,
     duplicate_tp_group: bool = False,
+    dp_size: int = 1,
 ) -> None:
     """
     Initialize model parallel groups.
@@ -1362,6 +1458,8 @@ def initialize_model_parallel(
         _TP.pynccl_comm.disabled = False
         _PDMUX_PREFILL_TP_GROUP.pynccl_comm.disabled = False
 
+    initialize_add_groups(backend, tensor_model_parallel_size, dp_size)
+
     moe_ep_size = expert_model_parallel_size
 
     moe_tp_size = tensor_model_parallel_size // moe_ep_size
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 45837786e..214646360 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -606,6 +606,7 @@ class ModelRunner:
                 pipeline_model_parallel_size=self.pp_size,
                 expert_model_parallel_size=self.moe_ep_size,
                 duplicate_tp_group=self.server_args.enable_pdmux,
+                dp_size=self.server_args.dp_size,
             )
             initialize_dp_attention(
                 enable_dp_attention=self.server_args.enable_dp_attention,
