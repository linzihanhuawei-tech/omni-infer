diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py
index 71fe73686..c12b8d8a5 100644
--- a/python/sglang/srt/distributed/parallel_state.py
+++ b/python/sglang/srt/distributed/parallel_state.py
@@ -581,6 +581,19 @@ class GroupCoordinator:
         torch.distributed.reduce_scatter(output, input_list, group=self.device_group)
         return output
 
+    def reduce_scatter_(
+        self,
+        input_: torch.Tensor,
+    ) -> None:
+        input_size = tuple(input_.size())
+        output = torch.empty(
+            (input_size[0] // self.world_size,) + input_size[1:],
+            dtype=input_.dtype,
+            device=input_.device,
+        )
+        torch.distributed.reduce_scatter_tensor(output, input_, group=self.device_group)
+        return output
+
     def _all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
         pynccl_comm = self.pynccl_comm
         if pynccl_comm is not None and not pynccl_comm.disabled:
@@ -1288,6 +1301,7 @@ def initialize_model_parallel(
     pipeline_model_parallel_size: int = 1,
     backend: Optional[str] = None,
     duplicate_tp_group: bool = False,
+    dp_size: int = 1,
 ) -> None:
     """
     Initialize model parallel groups.
@@ -1362,6 +1376,8 @@ def initialize_model_parallel(
         _TP.pynccl_comm.disabled = False
         _PDMUX_PREFILL_TP_GROUP.pynccl_comm.disabled = False
 
+    initialize_add_groups(backend, tensor_model_parallel_size, dp_size)
+
     moe_ep_size = expert_model_parallel_size
 
     moe_tp_size = tensor_model_parallel_size // moe_ep_size
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 45837786e..214646360 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -606,6 +606,7 @@ class ModelRunner:
                 pipeline_model_parallel_size=self.pp_size,
                 expert_model_parallel_size=self.moe_ep_size,
                 duplicate_tp_group=self.server_args.enable_pdmux,
+                dp_size=self.server_args.dp_size,
             )
             initialize_dp_attention(
                 enable_dp_attention=self.server_args.enable_dp_attention,