Subject: [PATCH] add new feature multi api server deployment
---
Index: python/sglang/srt/disaggregation/launch_lb.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/disaggregation/launch_lb.py b/python/sglang/srt/disaggregation/launch_lb.py
--- a/python/sglang/srt/disaggregation/launch_lb.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/disaggregation/launch_lb.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -44,16 +44,14 @@
         parser.add_argument(
             "--prefill",
             type=str,
-            default=[],
-            nargs="+",
             help="URLs for prefill servers",
+            action="append",
         )
         parser.add_argument(
             "--decode",
             type=str,
-            default=[],
-            nargs="+",
             help="URLs for decode servers",
+            action="append",
         )
         parser.add_argument(
             "--prefill-bootstrap-ports",
Index: python/sglang/srt/entrypoints/engine.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/entrypoints/engine.py b/python/sglang/srt/entrypoints/engine.py
--- a/python/sglang/srt/entrypoints/engine.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/entrypoints/engine.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -694,14 +694,16 @@
     )
 
     scheduler_procs = []
+
+    nnodes_per_tp_group = max(server_args.nnodes // server_args.pp_size, 1)
+    tp_size_per_node = server_args.tp_size // nnodes_per_tp_group
+
     if server_args.dp_size == 1:
         memory_saver_adapter = TorchMemorySaverAdapter.create(
             enable=server_args.enable_memory_saver
         )
         scheduler_pipe_readers = []
 
-        nnodes_per_tp_group = max(server_args.nnodes // server_args.pp_size, 1)
-        tp_size_per_node = server_args.tp_size // nnodes_per_tp_group
         tp_rank_range = range(
             tp_size_per_node * (server_args.node_rank % nnodes_per_tp_group),
             tp_size_per_node * (server_args.node_rank % nnodes_per_tp_group + 1),
@@ -736,23 +738,74 @@
                         None,
                     ),
                 )
+
+                with memory_saver_adapter.configure_subprocess():
+                    proc.start()
+                scheduler_procs.append(proc)
+                scheduler_pipe_readers.append(reader)
+    else:
+        if server_args.enable_multi_api_server:
+            if not server_args.enable_dp_attention:
+                raise RuntimeError(
+                    "Multi api server must set enable dp attention."
+                )
+            else:
+                memory_saver_adapter = TorchMemorySaverAdapter.create(
+                    enable=server_args.enable_memory_saver
+                )
+                scheduler_pipe_readers = []
+                nnodes_per_tp_group = max(server_args.nnodes // server_args.pp_size, 1)
+                attn_tp_size = server_args.tp_size // server_args.dp_size
+                attn_tp_rank_range = range(
+                    server_args.api_server_id * attn_tp_size,
+                    server_args.api_server_id * attn_tp_size + attn_tp_size,
+                )
+
+                pp_size_per_node = max(server_args.pp_size // server_args.nnodes, 1)
+                pp_rank_range = range(
+                    pp_size_per_node * (server_args.node_rank // nnodes_per_tp_group),
+                    pp_size_per_node * (server_args.node_rank // nnodes_per_tp_group + 1),
+                )
+                for pp_rank in pp_rank_range:
+                    for tp_rank in attn_tp_rank_range:
+                        reader, writer = mp.Pipe(duplex=False)
+                        gpu_id = (
+                            server_args.base_gpu_id
+                            + ((pp_rank % pp_size_per_node) * tp_size_per_node)
+                            + (tp_rank % tp_size_per_node) * server_args.gpu_id_step
+                        )
+                        moe_ep_rank = tp_rank // (server_args.tp_size // server_args.ep_size)
+                        proc = mp.Process(
+                            target=run_scheduler_process,
+                            args=(
+                                server_args,
+                                port_args,
+                                gpu_id,
+                                tp_rank,
+                                moe_ep_rank,
+                                pp_rank,
+                                server_args.api_server_id,
+                                writer,
+                                None,
+                            ),
+                        )
 
-                with memory_saver_adapter.configure_subprocess():
-                    proc.start()
-                scheduler_procs.append(proc)
-                scheduler_pipe_readers.append(reader)
-    else:
-        # Launch the data parallel controller
-        reader, writer = mp.Pipe(duplex=False)
-        scheduler_pipe_readers = [reader]
-        proc = mp.Process(
-            target=run_data_parallel_controller_process,
-            args=(server_args, port_args, writer),
-        )
-        proc.start()
-        scheduler_procs.append(proc)
+                        with memory_saver_adapter.configure_subprocess():
+                            proc.start()
+                        scheduler_procs.append(proc)
+                        scheduler_pipe_readers.append(reader)
+        else:
+            # Launch the data parallel controller
+            reader, writer = mp.Pipe(duplex=False)
+            scheduler_pipe_readers = [reader]
+            proc = mp.Process(
+                target=run_data_parallel_controller_process,
+                args=(server_args, port_args, writer),
+            )
+            proc.start()
+            scheduler_procs.append(proc)
 
-    if server_args.node_rank >= 1:
+    if server_args.node_rank >= 1 and not server_args.enable_multi_api_server:
         # In multi-node cases, non-zero rank nodes do not need to run tokenizer or detokenizer,
         # so they can just wait here.
 
Index: python/sglang/srt/managers/detokenizer_manager.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/managers/detokenizer_manager.py b/python/sglang/srt/managers/detokenizer_manager.py
--- a/python/sglang/srt/managers/detokenizer_manager.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/managers/detokenizer_manager.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -73,14 +73,24 @@
         server_args: ServerArgs,
         port_args: PortArgs,
     ):
+        self.server_args = server_args
         # Init inter-process communication
-        context = zmq.Context(2)
-        self.recv_from_scheduler = get_zmq_socket(
-            context, zmq.PULL, port_args.detokenizer_ipc_name, True
-        )
-        self.send_to_tokenizer = get_zmq_socket(
-            context, zmq.PUSH, port_args.tokenizer_ipc_name, False
-        )
+        if not self.server_args.enable_multi_api_server:
+            context = zmq.Context(2)
+            self.recv_from_scheduler = get_zmq_socket(
+                context, zmq.PULL, port_args.detokenizer_ipc_name, True
+            )
+            self.send_to_tokenizer = get_zmq_socket(
+                context, zmq.PUSH, port_args.tokenizer_ipc_name, False
+            )
+        else:
+            context = zmq.Context(2)
+            self.recv_from_local_scheduler = get_zmq_socket(
+                context, zmq.PULL, port_args.local_detokenizer_ipc_name, True
+            )
+            self.send_to_local_tokenizer = get_zmq_socket(
+                context, zmq.PUSH, port_args.local_tokenizer_ipc_name, False
+            )
 
         if server_args.skip_tokenizer_init:
             self.tokenizer = None
@@ -106,9 +116,14 @@
     def event_loop(self):
         """The event loop that handles requests"""
         while True:
-            recv_obj = self.recv_from_scheduler.recv_pyobj()
-            output = self._request_dispatcher(recv_obj)
-            self.send_to_tokenizer.send_pyobj(output)
+            if self.server_args.enable_multi_api_server:
+                recv_obj = self.recv_from_local_scheduler.recv_pyobj()
+                output = self._request_dispatcher(recv_obj)
+                self.send_to_local_tokenizer.send_pyobj(output)
+            else:
+                recv_obj = self.recv_from_scheduler.recv_pyobj()
+                output = self._request_dispatcher(recv_obj)
+                self.send_to_tokenizer.send_pyobj(output)
 
     def trim_matched_stop(
         self, output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool
Index: python/sglang/srt/managers/scheduler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
--- a/python/sglang/srt/managers/scheduler.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/managers/scheduler.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -260,29 +260,55 @@
             self.send_to_tokenizer = get_zmq_socket(
                 context, zmq.PUSH, port_args.tokenizer_ipc_name, False
             )
+            if self.server_args.enable_multi_api_server:
+                self.recv_from_local_tokenizer = get_zmq_socket(
+                    context, zmq.PULL, port_args.local_scheduler_input_ipc_name, False
+                )
+                self.send_to_local_tokenizer = get_zmq_socket(
+                    context, zmq.PUSH, port_args.local_tokenizer_ipc_name, False
+                )
             if server_args.skip_tokenizer_init:
                 # Directly send to the TokenizerManager
                 self.send_to_detokenizer = get_zmq_socket(
                     context, zmq.PUSH, port_args.tokenizer_ipc_name, False
                 )
+                self.send_to_local_detokenizer = get_zmq_socket(
+                    context, zmq.PUSH, port_args.local_tokenizer_ipc_name, False
+                )
             else:
                 # Send to the DetokenizerManager
+                if self.server_args.enable_multi_api_server:
+                    self.send_to_local_detokenizer = get_zmq_socket(
+                        context, zmq.PUSH, port_args.local_detokenizer_ipc_name, False
+                    )
                 self.send_to_detokenizer = get_zmq_socket(
                     context, zmq.PUSH, port_args.detokenizer_ipc_name, False
                 )
 
             if self.server_args.sleep_on_idle:
-                self.idle_sleeper = IdleSleeper(
-                    [
-                        self.recv_from_tokenizer,
-                        self.recv_from_rpc,
-                    ]
-                )
+                if not self.server_args.enable_multi_api_server:
+                    self.idle_sleeper = IdleSleeper(
+                        [
+                            self.recv_from_tokenizer,
+                            self.recv_from_rpc,
+                        ]
+                    )
+                else:
+                    self.idle_sleeper = IdleSleeper(
+                        [
+                            self.recv_from_local_tokenizer,
+                            self.recv_from_rpc,
+                        ]
+                    )
         else:
             self.recv_from_tokenizer = None
             self.recv_from_rpc = None
             self.send_to_tokenizer = SimpleNamespace(send_pyobj=lambda x: None)
             self.send_to_detokenizer = SimpleNamespace(send_pyobj=lambda x: None)
+            if self.server_args.enable_multi_api_server:
+                self.recv_from_local_tokenizer = None
+                self.send_to_local_tokenizer = SimpleNamespace(send_pyobj=lambda x: None)
+                self.send_to_local_detokenizer = SimpleNamespace(send_pyobj=lambda x: None)
 
         if self.current_scheduler_metrics_enabled():
             self.send_metrics_from_scheduler = get_zmq_socket(
@@ -964,18 +990,28 @@
                 recv_reqs = []
 
                 while True:
+                    count = 0
+                    try:
+                        if self.server_args.enable_multi_api_server:
+                            local_recv_req = self.recv_from_local_tokenizer.recv_pyobj(zmq.NOBLOCK)
+                            recv_reqs.append(local_recv_req)
+                    except zmq.ZMQError:
+                        count +=1
+                        pass
                     try:
                         recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
+                        recv_reqs.append(recv_req)
                     except zmq.ZMQError:
-                        break
-                    recv_reqs.append(recv_req)
+                        count +=1
+                        if count == 2 or not self.server_args.enable_multi_api_server:
+                            break
 
                 while True:
                     try:
                         recv_rpc = self.recv_from_rpc.recv_pyobj(zmq.NOBLOCK)
+                        recv_reqs.append(recv_rpc)
                     except zmq.ZMQError:
                         break
-                    recv_reqs.append(recv_rpc)
             else:
                 recv_reqs = None
         else:
@@ -1068,7 +1104,10 @@
                     if self.recv_from_rpc is not None:
                         self.recv_from_rpc.send_pyobj(output)
                 else:
-                    self.send_to_tokenizer.send_pyobj(output)
+                    if self.server_args.enable_multi_api_server:
+                        self.send_to_local_tokenizer.send_pyobj(output)
+                    else:
+                        self.send_to_tokenizer.send_pyobj(output)
 
     def handle_generate_request(
         self,
@@ -1804,7 +1843,10 @@
             # This is used to prevent the health check signal being blocked by long context prefill.
             # However, one minor issue is that this code path does not check the status of detokenizer manager.
             self.return_health_check_ct -= 1
-            self.send_to_tokenizer.send_pyobj(HealthCheckOutput())
+            if self.server_args.enable_multi_api_server:
+                self.send_to_local_tokenizer.send_pyobj(HealthCheckOutput())
+            else:
+                self.send_to_tokenizer.send_pyobj(HealthCheckOutput())
 
     def prepare_mlp_sync_batch(self, local_batch: ScheduleBatch):
         return self.prepare_mlp_sync_batch_raw(
Index: python/sglang/srt/managers/scheduler_output_processor_mixin.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/managers/scheduler_output_processor_mixin.py b/python/sglang/srt/managers/scheduler_output_processor_mixin.py
--- a/python/sglang/srt/managers/scheduler_output_processor_mixin.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/managers/scheduler_output_processor_mixin.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -669,37 +669,68 @@
         if rids:
             if self.model_config.is_multimodal_gen:
                 return
-
-            self.send_to_detokenizer.send_pyobj(
-                BatchTokenIDOut(
-                    rids,
-                    finished_reasons,
-                    decoded_texts,
-                    decode_ids_list,
-                    read_offsets,
-                    output_ids,
-                    skip_special_tokens,
-                    spaces_between_special_tokens,
-                    no_stop_trim,
-                    prompt_tokens,
-                    completion_tokens,
-                    cached_tokens,
-                    spec_verify_ct,
-                    input_token_logprobs_val,
-                    input_token_logprobs_idx,
-                    output_token_logprobs_val,
-                    output_token_logprobs_idx,
-                    input_top_logprobs_val,
-                    input_top_logprobs_idx,
-                    output_top_logprobs_val,
-                    output_top_logprobs_idx,
-                    input_token_ids_logprobs_val,
-                    input_token_ids_logprobs_idx,
-                    output_token_ids_logprobs_val,
-                    output_token_ids_logprobs_idx,
-                    output_hidden_states,
-                )
-            )
+            if self.server_args.enable_multi_api_server:
+                self.send_to_local_detokenizer.send_pyobj(
+                    BatchTokenIDOut(
+                        rids,
+                        finished_reasons,
+                        decoded_texts,
+                        decode_ids_list,
+                        read_offsets,
+                        output_ids,
+                        skip_special_tokens,
+                        spaces_between_special_tokens,
+                        no_stop_trim,
+                        prompt_tokens,
+                        completion_tokens,
+                        cached_tokens,
+                        spec_verify_ct,
+                        input_token_logprobs_val,
+                        input_token_logprobs_idx,
+                        output_token_logprobs_val,
+                        output_token_logprobs_idx,
+                        input_top_logprobs_val,
+                        input_top_logprobs_idx,
+                        output_top_logprobs_val,
+                        output_top_logprobs_idx,
+                        input_token_ids_logprobs_val,
+                        input_token_ids_logprobs_idx,
+                        output_token_ids_logprobs_val,
+                        output_token_ids_logprobs_idx,
+                        output_hidden_states,
+                    )
+                )
+            else:
+                self.send_to_detokenizer.send_pyobj(
+                    BatchTokenIDOut(
+                        rids,
+                        finished_reasons,
+                        decoded_texts,
+                        decode_ids_list,
+                        read_offsets,
+                        output_ids,
+                        skip_special_tokens,
+                        spaces_between_special_tokens,
+                        no_stop_trim,
+                        prompt_tokens,
+                        completion_tokens,
+                        cached_tokens,
+                        spec_verify_ct,
+                        input_token_logprobs_val,
+                        input_token_logprobs_idx,
+                        output_token_logprobs_val,
+                        output_token_logprobs_idx,
+                        input_top_logprobs_val,
+                        input_top_logprobs_idx,
+                        output_top_logprobs_val,
+                        output_top_logprobs_idx,
+                        input_token_ids_logprobs_val,
+                        input_token_ids_logprobs_idx,
+                        output_token_ids_logprobs_val,
+                        output_token_ids_logprobs_idx,
+                        output_hidden_states,
+                    )
+                )
 
     def stream_output_embedding(self: Scheduler, reqs: List[Req]):
         rids = []
@@ -715,8 +746,15 @@
                 embeddings.append(req.embedding)
                 prompt_tokens.append(len(req.origin_input_ids))
                 cached_tokens.append(req.cached_tokens)
-        self.send_to_detokenizer.send_pyobj(
-            BatchEmbeddingOut(
-                rids, finished_reasons, embeddings, prompt_tokens, cached_tokens
-            )
-        )
+        if self.server_args.enable_multi_api_server:
+            self.send_to_local_detokenizer.send_pyobj(
+                BatchEmbeddingOut(
+                    rids, finished_reasons, embeddings, prompt_tokens, cached_tokens
+                )
+            )
+        else:
+            self.send_to_detokenizer.send_pyobj(
+                BatchEmbeddingOut(
+                    rids, finished_reasons, embeddings, prompt_tokens, cached_tokens
+                )
+            )
Index: python/sglang/srt/managers/tokenizer_manager.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py
--- a/python/sglang/srt/managers/tokenizer_manager.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/managers/tokenizer_manager.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -255,14 +255,31 @@
                 )
 
         # Init inter-process communication
-        context = zmq.asyncio.Context(2)
-        self.recv_from_detokenizer = get_zmq_socket(
-            context, zmq.PULL, port_args.tokenizer_ipc_name, True
-        )
-        self.send_to_scheduler = get_zmq_socket(
-            context, zmq.PUSH, port_args.scheduler_input_ipc_name, True
-        )
-
+        if not self.server_args.enable_multi_api_server:
+            context = zmq.asyncio.Context(2)
+            self.recv_from_detokenizer = get_zmq_socket(
+                context, zmq.PULL, port_args.tokenizer_ipc_name, True
+            )
+            self.send_to_scheduler = get_zmq_socket(
+                context, zmq.PUSH, port_args.scheduler_input_ipc_name, True
+            )
+        else:
+            # Multi api server deployment only api server id 0 bind tokenizer, scheduler input
+            ctx = zmq.asyncio.Context(2)
+            self.recv_from_local_detokenizer = get_zmq_socket(
+                ctx, zmq.PULL, port_args.local_tokenizer_ipc_name, True
+            )
+            self.send_to_local_scheduler = get_zmq_socket(
+                ctx, zmq.PUSH, port_args.local_scheduler_input_ipc_name, True
+            )
+            if self.server_args.api_server_id == 0:
+                context = zmq.asyncio.Context(2)
+                self.recv_from_detokenizer = get_zmq_socket(
+                    context, zmq.PULL, port_args.tokenizer_ipc_name, True
+                )
+                self.send_to_scheduler = get_zmq_socket(
+                    context, zmq.PUSH, port_args.scheduler_input_ipc_name, True
+                )
         # Request states
         self.no_create_loop = False
         self.rid_to_state: Dict[str, ReqState] = {}
@@ -352,120 +369,140 @@
                 bucket_inter_token_latency=self.server_args.bucket_inter_token_latency,
                 collect_tokens_histogram=self.server_args.collect_tokens_histogram,
             )
-
-        # Communicators
-        self.init_weights_update_group_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.update_weights_from_distributed_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.update_weights_from_tensor_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.get_weights_by_name_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.release_memory_occupation_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.resume_memory_occupation_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.slow_down_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.flush_cache_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.profile_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.get_internal_state_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.set_internal_state_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.expert_distribution_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-        self.update_lora_adapter_communicator = _Communicator(
-            self.send_to_scheduler, server_args.dp_size
-        )
-
-        self._result_dispatcher = TypeBasedDispatcher(
-            [
-                (
-                    (
-                        BatchStrOut,
-                        BatchEmbeddingOut,
-                        BatchTokenIDOut,
-                        BatchMultimodalOut,
-                    ),
-                    self._handle_batch_output,
-                ),
-                (AbortReq, self._handle_abort_req),
-                (OpenSessionReqOutput, self._handle_open_session_req_output),
-                (
-                    UpdateWeightFromDiskReqOutput,
-                    self._handle_update_weights_from_disk_req_output,
-                ),
-                (
-                    InitWeightsUpdateGroupReqOutput,
-                    self.init_weights_update_group_communicator.handle_recv,
-                ),
-                (
-                    UpdateWeightsFromDistributedReqOutput,
-                    self.update_weights_from_distributed_communicator.handle_recv,
-                ),
-                (
-                    UpdateWeightsFromTensorReqOutput,
-                    self.update_weights_from_tensor_communicator.handle_recv,
-                ),
-                (
-                    GetWeightsByNameReqOutput,
-                    self.get_weights_by_name_communicator.handle_recv,
-                ),
-                (
-                    ReleaseMemoryOccupationReqOutput,
-                    self.release_memory_occupation_communicator.handle_recv,
-                ),
-                (
-                    ResumeMemoryOccupationReqOutput,
-                    self.resume_memory_occupation_communicator.handle_recv,
-                ),
-                (
-                    SlowDownReqOutput,
-                    self.slow_down_communicator.handle_recv,
-                ),
-                (
-                    FlushCacheReqOutput,
-                    self.flush_cache_communicator.handle_recv,
-                ),
-                (
-                    ProfileReqOutput,
-                    self.profile_communicator.handle_recv,
-                ),
-                (
-                    GetInternalStateReqOutput,
-                    self.get_internal_state_communicator.handle_recv,
-                ),
-                (
-                    SetInternalStateReqOutput,
-                    self.set_internal_state_communicator.handle_recv,
-                ),
-                (
-                    ExpertDistributionReqOutput,
-                    self.expert_distribution_communicator.handle_recv,
-                ),
-                (
-                    LoRAUpdateResult,
-                    self.update_lora_adapter_communicator.handle_recv,
-                ),
-                (HealthCheckOutput, lambda x: None),
-            ]
-        )
+        if self.server_args.api_server_id == 0:
+            # Communicators
+            self.init_weights_update_group_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.update_weights_from_distributed_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.update_weights_from_tensor_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.get_weights_by_name_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.release_memory_occupation_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.resume_memory_occupation_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.slow_down_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.flush_cache_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.profile_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.get_internal_state_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.set_internal_state_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.expert_distribution_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self.update_lora_adapter_communicator = _Communicator(
+                self.send_to_scheduler, server_args.dp_size
+            )
+            self._result_dispatcher = TypeBasedDispatcher(
+                [
+                    (
+                        (
+                            BatchStrOut,
+                            BatchEmbeddingOut,
+                            BatchTokenIDOut,
+                            BatchMultimodalOut,
+                        ),
+                        self._handle_batch_output,
+                    ),
+                    (AbortReq, self._handle_abort_req),
+                    (OpenSessionReqOutput, self._handle_open_session_req_output),
+                    (
+                        UpdateWeightFromDiskReqOutput,
+                        self._handle_update_weights_from_disk_req_output,
+                    ),
+                    (
+                        InitWeightsUpdateGroupReqOutput,
+                        self.init_weights_update_group_communicator.handle_recv,
+                    ),
+                    (
+                        UpdateWeightsFromDistributedReqOutput,
+                        self.update_weights_from_distributed_communicator.handle_recv,
+                    ),
+                    (
+                        UpdateWeightsFromTensorReqOutput,
+                        self.update_weights_from_tensor_communicator.handle_recv,
+                    ),
+                    (
+                        GetWeightsByNameReqOutput,
+                        self.get_weights_by_name_communicator.handle_recv,
+                    ),
+                    (
+                        ReleaseMemoryOccupationReqOutput,
+                        self.release_memory_occupation_communicator.handle_recv,
+                    ),
+                    (
+                        ResumeMemoryOccupationReqOutput,
+                        self.resume_memory_occupation_communicator.handle_recv,
+                    ),
+                    (
+                        SlowDownReqOutput,
+                        self.slow_down_communicator.handle_recv,
+                    ),
+                    (
+                        FlushCacheReqOutput,
+                        self.flush_cache_communicator.handle_recv,
+                    ),
+                    (
+                        ProfileReqOutput,
+                        self.profile_communicator.handle_recv,
+                    ),
+                    (
+                        GetInternalStateReqOutput,
+                        self.get_internal_state_communicator.handle_recv,
+                    ),
+                    (
+                        SetInternalStateReqOutput,
+                        self.set_internal_state_communicator.handle_recv,
+                    ),
+                    (
+                        ExpertDistributionReqOutput,
+                        self.expert_distribution_communicator.handle_recv,
+                    ),
+                    (
+                        LoRAUpdateResult,
+                        self.update_lora_adapter_communicator.handle_recv,
+                    ),
+                    (HealthCheckOutput, lambda x: None),
+                ]
+            )
+        else:
+            self._result_dispatcher = TypeBasedDispatcher(
+                [
+                    (
+                        (
+                            BatchStrOut,
+                            BatchEmbeddingOut,
+                            BatchTokenIDOut,
+                            BatchMultimodalOut,
+                        ),
+                        self._handle_batch_output,
+                    ),
+                    (AbortReq, self._handle_abort_req),
+                    (OpenSessionReqOutput, self._handle_open_session_req_output),
+                    (
+                        UpdateWeightFromDiskReqOutput,
+                        self._handle_update_weights_from_disk_req_output,
+                    ),
+                    (HealthCheckOutput, lambda x: None),
+                ]
+            )
 
     async def generate_request(
         self,
@@ -734,7 +771,10 @@
         tokenized_obj: Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput],
         created_time: Optional[float] = None,
     ):
-        self.send_to_scheduler.send_pyobj(tokenized_obj)
+        if self.server_args.enable_multi_api_server:
+            self.send_to_local_scheduler.send_pyobj(tokenized_obj)
+        else:
+            self.send_to_scheduler.send_pyobj(tokenized_obj)
         state = ReqState([], False, asyncio.Event(), obj, created_time=created_time)
         self.rid_to_state[obj.rid] = state
         return state
@@ -845,8 +885,12 @@
                     rids.append(tmp_obj.rid)
             else:
                 # Sequential tokenization and processing
+                if self.server_args.enable_multi_api_server:
+                    tokenize = self.send_to_local_scheduler
+                else:
+                    tokenize = self.send_to_scheduler
                 with (
-                    input_blocker_guard_region(send_to_scheduler=self.send_to_scheduler)
+                    input_blocker_guard_region(send_to_scheduler=tokenize)
                     if get_bool_env_var("SGLANG_ENABLE_COLOCATED_BATCH_GEN")
                     else nullcontext()
                 ):
@@ -927,7 +971,10 @@
         if not abort_all and rid not in self.rid_to_state:
             return
         req = AbortReq(rid, abort_all)
-        self.send_to_scheduler.send_pyobj(req)
+        if self.server_args.enable_multi_api_server:
+            self.send_to_local_scheduler.send_pyobj(req)
+        else:
+            self.send_to_scheduler.send_pyobj(req)
 
         if self.enable_metrics:
             self.metrics_collector.observe_one_aborted_request()
@@ -1015,7 +1062,10 @@
     async def _wait_for_model_update_from_disk(
         self, obj: UpdateWeightFromDiskReqInput
     ) -> Tuple[bool, str]:
-        self.send_to_scheduler.send_pyobj(obj)
+        if self.server_args.enable_multi_api_server:
+            self.send_to_local_scheduler.send_pyobj(obj)
+        else:
+            self.send_to_scheduler.send_pyobj(obj)
         self.model_update_result = asyncio.Future()
         if self.server_args.dp_size == 1:
             result = await self.model_update_result
@@ -1507,10 +1557,14 @@
         """The event loop that handles requests"""
 
         while True:
-            recv_obj = await self.recv_from_detokenizer.recv_pyobj()
-            self._result_dispatcher(recv_obj)
-            self.last_receive_tstamp = time.time()
-
+            if self.server_args.enable_multi_api_server:
+                recv_obj = await self.recv_from_local_detokenizer.recv_pyobj()
+                self._result_dispatcher(recv_obj)
+                self.last_receive_tstamp = time.time()
+            else:
+                recv_obj = await self.recv_from_detokenizer.recv_pyobj()
+                self._result_dispatcher(recv_obj)
+                self.last_receive_tstamp = time.time()
     def _handle_batch_output(
         self,
         recv_obj: Union[
Index: python/sglang/srt/server_args.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
--- a/python/sglang/srt/server_args.py	(revision e6fcf3f656eab95e8d3c57a6c08ecf10b4e77aa7)
+++ b/python/sglang/srt/server_args.py	(revision 13a8669b7a5acef0f19dc3f804d8ef0fa402f88a)
@@ -102,6 +102,7 @@
     dist_timeout: Optional[int] = None  # timeout for torch.distributed
     download_dir: Optional[str] = None
     base_gpu_id: int = 0
+    api_server_id: int = 0
     gpu_id_step: int = 1
     sleep_on_idle: bool = False
 
@@ -229,6 +230,7 @@
     disable_overlap_schedule: bool = False
     enable_mixed_chunk: bool = False
     enable_dp_attention: bool = False
+    enable_multi_api_server: bool = False
     enable_dp_lm_head: bool = False
     enable_two_batch_overlap: bool = False
     tbo_token_distribution_threshold: float = 0.48
@@ -1019,6 +1021,12 @@
             default=ServerArgs.tp_size,
             help="The tensor parallelism size.",
         )
+        parser.add_argument(
+            "--api-server-id",
+            type=int,
+            default=ServerArgs.api_server_id,
+            help="Current api server id, which means dp_rank.",
+        )
         parser.add_argument(
             "--pipeline-parallel-size",
             "--pp-size",
@@ -1749,6 +1757,11 @@
             action="store_true",
             help="Enabling data parallelism for attention and tensor parallelism for FFN. The dp size should be equal to the tp size. Currently DeepSeek-V2 and Qwen 2/3 MoE models are supported.",
         )
+        parser.add_argument(
+            "--enable-multi-api-server",
+            action="store_true",
+            help="Enabling multi api server deployment.",
+        )
         parser.add_argument(
             "--enable-dp-lm-head",
             action="store_true",
@@ -2255,6 +2268,13 @@
     # The ipc filename for Scheduler to send metrics
     metrics_ipc_name: str
 
+    # Local tokenizer ipc for multi api server
+    local_tokenizer_ipc_name: str = None
+    # Local scheduler input ipc for multi api server
+    local_scheduler_input_ipc_name: str = None
+    # Local detokenizer ipc for multi api server
+    local_detokenizer_ipc_name: str = None
+
     @staticmethod
     def init_new(server_args, dp_rank: Optional[int] = None) -> "PortArgs":
         if server_args.nccl_port is None:
@@ -2304,7 +2324,7 @@
             else:
                 scheduler_input_port = port_base + 4 + 1 + dp_rank
 
-            return PortArgs(
+            port_args = PortArgs(
                 tokenizer_ipc_name=f"tcp://{dist_init_host}:{port_base}",
                 scheduler_input_ipc_name=f"tcp://{dist_init_host}:{scheduler_input_port}",
                 detokenizer_ipc_name=f"tcp://{dist_init_host}:{detokenizer_port}",
@@ -2313,6 +2333,15 @@
                 metrics_ipc_name=f"tcp://{dist_init_host}:{metrics_ipc_name}",
             )
 
+            if server_args.enable_multi_api_server:
+                suf = f"_api_server_id{server_args.api_server_id}"
+                port_args.local_detokenizer_ipc_name = \
+                    f"ipc://{tempfile.NamedTemporaryFile(delete=False, suffix=suf).name}"
+                port_args.local_scheduler_input_ipc_name = \
+                    f"ipc://{tempfile.NamedTemporaryFile(delete=False, suffix=suf).name}"
+                port_args.local_tokenizer_ipc_name = \
+                    f"ipc://{tempfile.NamedTemporaryFile(delete=False, suffix=suf).name}"
+            return port_args
 
 class LoRAPathAction(argparse.Action):
     def __call__(self, parser, namespace, values, option_string=None):
