diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 81aa039bf..e86fbd6c1 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1215,10 +1215,10 @@ class EngineArgs:
                                recommend_to_remove=False)
             return False
 
-        if self.preemption_mode != SchedulerConfig.preemption_mode:
-            _raise_or_fallback(feature_name="--preemption-mode",
-                               recommend_to_remove=True)
-            return False
+        # if self.preemption_mode != SchedulerConfig.preemption_mode:
+        #     _raise_or_fallback(feature_name="--preemption-mode",
+        #                        recommend_to_remove=True)
+        #     return False
 
         if (self.disable_async_output_proc
                 != EngineArgs.disable_async_output_proc):
diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 74313be9c..8a1a91f2a 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -6,7 +6,7 @@ from typing import Optional
 
 from vllm.distributed.kv_events import KVCacheEvent
 from vllm.logger import init_logger
-from vllm.utils import sha256
+from vllm.utils import (sha256, Device)
 from vllm.v1.core.block_pool import BlockPool
 from vllm.v1.core.kv_cache_utils import (BlockHashType, KVCacheBlock,
                                          hash_request_tokens)
@@ -177,6 +177,7 @@ class KVCacheManager:
         num_draft_tokens: int = 0,
         num_lookahead_tokens: int = 0,
         delay_cache_blocks: bool = False,
+        is_swap: bool = False
     ) -> Optional[KVCacheBlocks]:
         """Add slots for a request with new tokens to append.
 
@@ -213,7 +214,7 @@ class KVCacheManager:
         Returns:
             A list of new allocated blocks.
         """
-        if num_new_tokens == 0:
+        if num_new_tokens == 0 and not is_swap:
             raise ValueError("num_new_tokens must be greater than 0")
 
         if new_computed_blocks is not None:
@@ -372,3 +373,91 @@ class KVCacheManager:
             return [[]]
         return KVCacheBlocks(self.single_type_manager.req_to_blocks[request_id]
                              ).get_block_ids()
+
+class CpuNpuKVCacheManager:
+
+    def __init__(self, npu_cache_manager: KVCacheManager, cpu_cache_manager: KVCacheManager, max_model_len: int, enable_omni_attn: bool) -> None:
+        self.allocators = {
+            Device.GPU: npu_cache_manager,
+            Device.CPU: cpu_cache_manager
+        }
+        self.max_model_len = max_model_len
+        self.enable_omni_attn = enable_omni_attn
+
+    def _can_swap(self, device: Device, num_need_blocks: list[int]) -> bool:
+        if self.enable_omni_attn:
+            num_free_blocks = [bp.get_num_free_blocks() for bp in self.allocators[device].block_pools]
+        else:
+            num_free_blocks = [self.allocators[device].single_type_manager.block_pool.get_num_free_blocks()]
+        if any(need > free for need, free in zip(num_need_blocks, num_free_blocks)):
+            return False
+        return True
+
+    def can_swap_in(self, request: Request, num_new_tokens: int, num_new_computed_tokens: int = 0, num_lookahead_tokens: int = 0) -> bool:
+        num_computed_tokens = (request.num_computed_tokens +
+                               num_new_computed_tokens)
+        num_tokens_need_slot = min(
+            num_computed_tokens + num_new_tokens + num_lookahead_tokens,
+            self.max_model_len)
+        if self.enable_omni_attn:
+            num_blocks_to_allocate: list[int] = [
+                mgr.get_num_blocks_to_allocate(
+                    request_id=request.request_id,
+                    num_tokens=num_tokens_need_slot,
+                    new_computed_blocks=[],
+                ) for mgr in self.allocators[Device.GPU].hybrid_managers]
+        else:
+            num_blocks_to_allocate = [
+                self.allocators[Device.GPU].single_type_manager.get_num_blocks_to_allocate(
+                    request_id=request.request_id,
+                    num_tokens=num_tokens_need_slot,
+                    new_computed_blocks=[],
+                )]
+        return self._can_swap(Device.GPU, num_blocks_to_allocate)
+
+    def can_swap_out(self, request: Request) -> bool:
+        if self.enable_omni_attn:
+            num_cached_blocks = [len(mgr.req_to_blocks[request.request_id]) for mgr in self.allocators[Device.GPU].hybrid_managers]
+        else:
+            num_cached_blocks = [len(self.allocators[Device.GPU].single_type_manager.req_to_blocks[request.request_id])]
+        return self._can_swap(Device.CPU, num_cached_blocks)
+
+    def swap(self,request: Request, src_device: Device, dst_device: Device)-> list[list[tuple[int, int]]]:
+        raw_src_block_ids = self._swap_out(request, src_device)
+        raw_dst_block_ids = self._swap_in(request, dst_device)
+        if not raw_src_block_ids or not raw_dst_block_ids:
+            return []
+        block_id_mapping = [
+            list(zip(src_block_ids, dst_block_ids))
+            for src_block_ids, dst_block_ids in zip(raw_src_block_ids, raw_dst_block_ids)
+        ]
+        return block_id_mapping
+
+    def _swap_in(self,request: Request, device: Device) -> list[list[int]]:
+        raw_kv_cache_blocks = self.allocators[device].allocate_slots(request, 0, is_swap=True)
+        if raw_kv_cache_blocks is None:
+            return []
+        kv_cache_blocks = raw_kv_cache_blocks.blocks
+        if not self.enable_omni_attn:
+           kv_cache_blocks = [kv_cache_blocks]
+           
+        dst_block_ids = [
+            [block.block_id for block in blocks]
+            for blocks in kv_cache_blocks
+        ]
+        return dst_block_ids
+
+    def _swap_out(self,request: Request, device: Device) -> list[list[int]]:
+        if self.enable_omni_attn:
+            src_blocks = [mgr.req_to_blocks[request.request_id] for mgr in self.allocators[Device.GPU].hybrid_managers]
+        else:
+            src_blocks = [self.allocators[device].single_type_manager.req_to_blocks[request.request_id]]
+        src_block_ids = [
+            [block.block_id for block in blocks]
+            for blocks in src_blocks
+        ]
+        self.allocators[device].free(request)
+        return src_block_ids
+
+    def free(self, request: Request, device: Device) -> None:
+        self.allocators[device].free(request)
\ No newline at end of file
diff --git a/vllm/v1/core/sched/output.py b/vllm/v1/core/sched/output.py
index 3ed7dc664..e7c95f890 100644
--- a/vllm/v1/core/sched/output.py
+++ b/vllm/v1/core/sched/output.py
@@ -152,4 +152,6 @@ class SchedulerOutput:
     # KV Cache Connector metadata.
     kv_connector_metadata: Optional[KVConnectorMetadata] = None
     # Number of steps to schedule
-    num_step: Optional[int] = 1
\ No newline at end of file
+    num_step: Optional[int] = 1
+    blocks_to_swap_in: list[list[tuple[int, int]]] = None
+    blocks_to_swap_out: list[list[tuple[int, int]]] = None
\ No newline at end of file
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index ffebd194c..08eb4ea8a 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -5,6 +5,8 @@ from __future__ import annotations
 import time
 from collections import defaultdict, deque
 from collections.abc import Iterable
+import copy
+import enum
 from typing import Any, Optional, Union
 
 from vllm.config import VllmConfig
@@ -14,17 +16,18 @@ from vllm.distributed.kv_transfer.kv_connector.factory import (
 from vllm.distributed.kv_transfer.kv_connector.v1 import (KVConnectorBase_V1,
                                                           KVConnectorRole)
 from vllm.logger import init_logger
+from vllm.utils import Device
 from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry
 from vllm.v1.core.encoder_cache_manager import (EncoderCacheManager,
                                                 compute_encoder_budget)
-from vllm.v1.core.kv_cache_manager import KVCacheBlocks, KVCacheManager
+from vllm.v1.core.kv_cache_manager import KVCacheBlocks, KVCacheManager, CpuNpuKVCacheManager
 from vllm.v1.core.sched.interface import SchedulerInterface
 from vllm.v1.core.sched.output import (CachedRequestData, NewRequestData,
                                        SchedulerOutput)
 from vllm.v1.core.sched.utils import check_stop
 from vllm.v1.engine import (EngineCoreEventType, EngineCoreOutput,
                             EngineCoreOutputs)
-from vllm.v1.kv_cache_interface import KVCacheConfig
+from vllm.v1.kv_cache_interface import KVCacheConfig, FullAttentionSpec
 from vllm.v1.metrics.stats import SchedulerStats
 from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
@@ -37,6 +40,18 @@ import os
 reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
 FORCE_ENABLE_CHUNK_PREFILL = os.getenv("FORCE_ENABLE_CHUNK_PREFILL", "0") == "1"
 
+class PreemptionMode(enum.Enum):
+    SWAP = enum.auto()
+    RECOMPUTE = enum.auto()
+
+def to_bool_or_raise(val) -> bool:
+    if isinstance(val, bool):
+       return val
+    if isinstance(val, int):
+        return val == 1
+    if isinstance(val, str):
+        return val.lower() in ["1", "true"]
+    raise ValueError(f"Cannot convert variable to bool. Type {type(val)}. Value {val}.")
 
 class Scheduler(SchedulerInterface):
 
@@ -59,9 +74,11 @@ class Scheduler(SchedulerInterface):
         self.structured_output_manager = structured_output_manager
         additional_config = vllm_config.additional_config
         self.async_schedule = False
+        self.enable_omni_attn = False
         if additional_config:
             self.async_schedule = additional_config.get(
                     "async_schedule", False)
+            self.enable_omni_attn = to_bool_or_raise(additional_config["enable_omni_attn"])
 
         # include_finished_set controls whether a separate set of finished
         # request ids should be included in the EngineCoreOutputs returned
@@ -157,6 +174,27 @@ class Scheduler(SchedulerInterface):
             enable_kv_cache_events=self.enable_kv_cache_events,
         )
 
+        self.preemption_mode = PreemptionMode.RECOMPUTE
+        if self.scheduler_config.preemption_mode == "swap":
+            self.preemption_mode = PreemptionMode.SWAP
+            self.cpu_kv_cache_config = copy.deepcopy(kv_cache_config)
+            for kv_cache_group in self.cpu_kv_cache_config.kv_cache_groups:
+                cpu_kv_cache_spec = kv_cache_group.kv_cache_spec
+                cpu_num_blocks = int(self.vllm_config.cache_config.swap_space_bytes //
+                                           cpu_kv_cache_spec.page_size_bytes // len(self.cpu_kv_cache_config.tensors))
+                self.cpu_kv_cache_config.num_blocks = cpu_num_blocks
+                if self.enable_omni_attn:
+                    from omni.accelerators.cache.kv_cache_interface import OmniAttentionSpec
+                    self.cpu_kv_cache_config.num_blocks_per_group = {FullAttentionSpec: cpu_num_blocks, OmniAttentionSpec: cpu_num_blocks}
+
<<<<<<< HEAD
+            cpu_kv_cache_manager = KVCacheManager(
+                kv_cache_config=kv_cache_config,
+                max_model_len=self.max_model_len,
+                enable_caching=False,
+            )
=======
+                cpu_kv_cache_manager = KVCacheManager(
+                    kv_cache_config=self.cpu_kv_cache_config,
+                    max_model_len=self.max_model_len,
+                    enable_caching=False,
+                )
>>>>>>> 910ebaf (feat:Adapt to omni attention)
+
+                self.cpu_npu_kv_cache_manager = CpuNpuKVCacheManager(self.kv_cache_manager, cpu_kv_cache_manager, self.max_model_len, self.enable_omni_attn)
+
     def schedule(self) -> SchedulerOutput:
         # NOTE(woosuk) on the scheduling algorithm:
         # There's no "decoding phase" nor "prefill phase" in the scheduler.
@@ -173,6 +211,7 @@ class Scheduler(SchedulerInterface):
         scheduled_resumed_reqs: list[Request] = []
         scheduled_running_reqs: list[Request] = []
         preempted_reqs: list[Request] = []
+        swap_reqs: list[Request] = []
 
         # NOTE: structured_output_request_ids maps
         # a request's (request that uses structured output)
@@ -191,6 +230,11 @@ class Scheduler(SchedulerInterface):
         # Spec decode-related.
         scheduled_spec_decode_tokens: dict[str, list[int]] = {}
 
+        # Blocks that need to be swapped or copied before model execution.
+        num_groups = len(self.kv_cache_config.kv_cache_groups)
+        blocks_to_swap_out: list[list[tuple[int, int]]] = [[] for _ in range(num_groups)]
+        blocks_to_swap_in: list[list[tuple[int, int]]] = [[] for _ in range(num_groups)]
+
         # For logging.
         scheduled_timestamp = time.monotonic()
 
@@ -259,9 +303,19 @@ class Scheduler(SchedulerInterface):
                     # The request cannot be scheduled.
                     # Preempt the lowest-priority request.
                     preempted_req = self.running.pop()
-                    self.kv_cache_manager.free(preempted_req)
+                    if self.preemption_mode == PreemptionMode.SWAP:
+                        if not self.cpu_npu_kv_cache_manager.can_swap_out(preempted_req):
+                            preempted_req.status = RequestStatus.FINISHED_LENGTH_CAPPED
+                            self.finish_requests(preempted_req.request_id, RequestStatus.FINISHED_ABORTED)
+                            logger.info(f"Preempting request {preempted_req.request_id} has been released.")
+                            can_schedule = False
+                            break
+                        self._swap_out(preempted_req, blocks_to_swap_out)
+                        logger.info(f"Request {request.request_id} swapped out, blocks_to_swap_out:{blocks_to_swap_out}")
+                    else:
+                        self.kv_cache_manager.free(preempted_req)
+                        preempted_req.num_computed_tokens = 0
                     preempted_req.status = RequestStatus.PREEMPTED
-                    preempted_req.num_computed_tokens = 0
                     if self.log_stats:
                         preempted_req.record_event(
                             EngineCoreEventType.PREEMPTED, scheduled_timestamp)
@@ -428,6 +482,12 @@ class Scheduler(SchedulerInterface):
                         if num_new_tokens == 0:
                             # The request cannot be scheduled.
                             break
+                if self.preemption_mode == PreemptionMode.SWAP and request.status == RequestStatus.PREEMPTED:
+                    if not self.cpu_npu_kv_cache_manager.can_swap_in(request, num_new_tokens, num_new_local_computed_tokens, self.num_lookahead_tokens):
+                        logger.info(f"Request {request.request_id} cannot be swapped in")
+                        break
+                    self._swap_in(request, blocks_to_swap_in)
+                    logger.info(f"Request {request.request_id} swapped in, blocks_to_swap_in:{blocks_to_swap_in}")
 
                 new_blocks = self.kv_cache_manager.allocate_slots(
                     request,
@@ -440,7 +500,6 @@ class Scheduler(SchedulerInterface):
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     break
-
                 # KVConnector: update internal state after allocation.
                 # This information is used to determine if a load is
                 # needed for this request.
@@ -575,6 +634,8 @@ class Scheduler(SchedulerInterface):
             free_encoder_input_ids=self.encoder_cache_manager.get_freed_ids(),
             structured_output_request_ids=structured_output_request_ids,
             grammar_bitmask=grammar_bitmask,
+            blocks_to_swap_in=blocks_to_swap_in,
+            blocks_to_swap_out=blocks_to_swap_out,
         )
 
         # NOTE(Kuntai): this function is designed for multiple purposes:
@@ -605,6 +666,28 @@ class Scheduler(SchedulerInterface):
         self.finished_req_ids = set()
         return scheduler_output
 
+    def _swap_in(
+            self,
+            req: Request,
+            blocks_to_swap_in: list[list[tuple[int, int]]],
+    ) -> None:
+        mapping_list = self.cpu_npu_kv_cache_manager.swap(req, Device.CPU, Device.GPU)
+        if not mapping_list:
+            return
+        for i in range(len(self.cpu_kv_cache_config.kv_cache_groups)):
+            blocks_to_swap_in[i].extend(mapping_list[i])
+
+    def _swap_out(
+            self,
+            req: Request,
+            blocks_to_swap_out: list[list[tuple[int, int]]],
+    ) -> None:
+        mapping_list = self.cpu_npu_kv_cache_manager.swap(req, Device.GPU, Device.CPU)
+        if not mapping_list:
+            return
+        for i in range(len(self.cpu_kv_cache_config.kv_cache_groups)):
+            blocks_to_swap_out[i].extend(mapping_list[i])
+
     def _make_cached_request_data(
         self,
         request: Request,
@@ -943,6 +1026,10 @@ class Scheduler(SchedulerInterface):
     def _free_blocks(self, request: Request):
         assert request.is_finished()
         assert request.request_id not in self._cached_reqs_data
+        if self.scheduler_config.preemption_mode == "swap":
+           logger.info("Request {request.request_id} free cpu blocks")
+           self.cpu_npu_kv_cache_manager.free(request, Device.CPU)
+
         self.kv_cache_manager.free(request)
         self.kv_cache_manager.free_block_hashes(request)
         del self.requests[request.request_id]
