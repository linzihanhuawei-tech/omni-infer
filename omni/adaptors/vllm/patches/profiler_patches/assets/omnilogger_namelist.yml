type: "marker"
targets:
  - module: "vllm.entrypoints.openai.serving_engine:OpenAIServing"
    function_name: _tokenize_prompt_input
    entry_operation: |
      import os, time
      raw_request_id = args[0].raw_request_id
      if os.getenv('ROLE') == "prefill":
          print(f"<<<Action: Get prefill engine request and start pickle; Timestamp:{time.time()}; RequestID:{raw_request_id}; Role:{os.getenv('ROLE')}")
    exit_operation: |
      import time, inspect
      raw_request_id = args[0].raw_request_id
      if os.getenv('ROLE') == "prefill":
          print(f"<<<Action: Finish process request in prefill engine; Timestamp:{time.time()}; RequestID:{raw_request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.entrypoints.openai.serving_chat:OpenAIServingChat"
    function_name: create_chat_completion
    entry_operation: |
      import os, time
      raw_request_id = "chatcmpl-" + args[1].headers.get('X-Request-Id')
      args[0].raw_request_id = raw_request_id
      if os.getenv('ROLE') == "prefill":
          print(f"<<<Action: PD api server get request; Timestamp:{time.time()}; RequestID:{raw_request_id}; Role:{os.getenv('ROLE')}")
      elif os.getenv('ROLE') == "decode":
          print(f"<<<Action: Enter decode to generate; Timestamp:{time.time()}; RequestID:{raw_request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.entrypoints.openai.serving_chat:OpenAIServingChat"
    function_name: chat_completion_stream_generator
    exit_operation: |
      import os, time
      raw_request_id = args[0].raw_request_id
      if os.getenv('ROLE') == "decode":
          print(f"<<<Action: Finish decode pickle and start response; Timestamp:{time.time()}; RequestID:{raw_request_id}; Role:{os.getenv('ROLE')}")
          
  - module: "vllm.v1.core.sched.scheduler:Scheduler"
    function_name: add_request
    entry_operation: |
      import os, time
      if os.getenv('ROLE') == "prefill":
          print(f"<<<Action: Prefill add waiting queue; Timestamp:{time.time()}; RequestID:{args[0].request_id}; Role:{os.getenv('ROLE')}")

 # Prefill free kv blocks
  - module: "vllm.v1.core.sched.scheduler:Scheduler"
    function_name: _update_from_kv_xfer_finished
    entry_operation: |
      import os, time
      for req_id in (args[0].finished_sending or ()):
          if req_id in self.requests and req_id not in self._cached_reqs_data:
              print(f"<<<Action: Prefill free kv blocks; Timestamp:{time.time()}; RequestID:{req_id}; Role:{os.getenv('ROLE')}")

  # start pull kv, finish pull kv
  - module: "omni.accelerators.pd.llmdatadist_connector_v1:DecodeConnectorWorker"
    function_name: _read_blocks
    entry_operation: |
      import os, time
      request_id = kwargs['request_id']
      print(f"<<<Action: Start pull kv; Timestamp:{time.time()}; RequestID:{request_id}; Role:{os.getenv('ROLE')}")
    exit_operation: |
      import os, time
      request_id = kwargs['request_id']
      print(f"<<<Action: Finish pull kv; Timestamp:{time.time()}; RequestID:{request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.v1.engine.core_client:AsyncMPClient"
    function_name: get_output_async
    exit_operation: |
      import os, time
      from vllm.v1.engine import EngineCoreOutputs
      if isinstance(result, EngineCoreOutputs) and os.getenv('ROLE') == "prefill":
          for request in result.outputs:
              print(f"<<<Action: Pop output queues; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")
              print(f"<<<Action: Finish prefill pickle and start response; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.v1.core.kv_cache_manager:KVCacheManager"
    function_name: allocate_slots
    exit_operation: |
      import os, time
      load_kv_async = False
      if "delay_cache_blocks" in kwargs:
          load_kv_async = kwargs['delay_cache_blocks']
      if result and load_kv_async == False and os.getenv('ROLE') == "prefill": # block is not none
          print(f"<<<Action: Prefill get new_blocks; Timestamp:{time.time()}; RequestID:{args[0].request_id}; Role:{os.getenv('ROLE')}")
      elif result and load_kv_async: #delay_cache_blocks is True
          print(f"<<<Action: Add need pulling sequence; Timestamp:{time.time()}; RequestID:{args[0].request_id}; Role:{os.getenv('ROLE')}")
      else: # new_block is None
          print(f"<<<Action: fail to add result of kv insufficient; Timestamp:{time.time()}; RequestID:{args[0].request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.v1.core.sched.scheduler:Scheduler"
    function_name: schedule
    entry_operation: |
      import os, time
      class CustomList(list):
          def append(self, item):
              if os.getenv('ROLE') == "prefill":
                  print(f"<<<Action: success add to seq groups; Timestamp:{time.time()}; RequestID:{item.request_id}; Role:{os.getenv('ROLE')}")
              else:
                  print(f"<<<Action: Start append running sequece for decode; Timestamp:{time.time()}; RequestID:{item.request_id}; Role:{os.getenv('ROLE')}")
              super().append(item)
      self.running = CustomList(self.running)
      for request in self.waiting: 
          print(f"<<<Action: try to schedule in waiting queue; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.v1.engine.core:EngineCore"
    function_name: execute_model
    entry_operation: |
      import os, time
      for req in args[0].scheduled_new_reqs:
          if os.getenv('ROLE') == "prefill":
              print(f"<<<Action: Start engine step; Timestamp:{time.time()}; RequestID:{req.req_id}; Role:{os.getenv('ROLE')}")
              print(f"<<<Action: Prefill start execute_model; Timestamp:{time.time()}; RequestID:{req.req_id}; Role:{os.getenv('ROLE')}")
          else: 
              print(f"<<<Action: Start to send output; Timestamp:{time.time()}; RequestID:{req.req_id}; Role:{os.getenv('ROLE')}")
    exit_operation: |
      import os, time
      for req in args[0].scheduled_new_reqs:
          if os.getenv('ROLE') == "prefill":
              print(f"<<<Action: Prefill done execute_model; Timestamp:{time.time()}; RequestID:{req.req_id}; Role:{os.getenv('ROLE')}")
              print(f"<<<Action: Finish engine step; Timestamp:{time.time()}; RequestID:{req.req_id}; Role:{os.getenv('ROLE')}")


  #### 以下开启会导致benchmark阻塞无法返回结果，需要定位问题
  # - module: "vllm.v1.engine.core_client:AsyncMPClient"
  #   function_name: _ensure_output_queue_task
  #   entry_operation: |
  #     import os, time
  #     def patch_queue_put_nowait(queue):
  #         original_put_nowait = queue.put_nowait
  #         def patched_put_nowait(item):
  #             result = original_put_nowait(item)
  #             if os.getenv('ROLE') == "prefill":
  #                 # Assuming item has similar structure to the example (with outputs and request_id)
  #                 for request in getattr(item, 'outputs', []):
  #                     print(f"<<<Action: Client get prefill output; Timestamp:{time.time()}; RequestID:{getattr(request, 'request_id', 'N/A')}; Role:{os.getenv('ROLE')}")
  #             return result
  #         queue.put_nowait = patched_put_nowait
  #         return queue
  #     self.outputs_queue = patch_queue_put_nowait(self.outputs_queue)

  # No output, need fix
  - module: "vllm.v1.engine.core:EngineCoreProc"
    function_name: process_input_socket
    entry_operation: |
      import os, time
      def patch_queue_put_nowait(queue):
          original_put_nowait = queue.put_nowait
          def patched_put_nowait(item):
              result = original_put_nowait(item)
              for request in getattr(item, 'outputs', []):
                  if os.getenv('ROLE') == "prefill":
                      print(f"<<<Action: Start process request in prefill engine; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")
                  else:
                      print(f"<<<Action: Start to dispatch decode request; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")
              return result
          queue.put_nowait = patched_put_nowait
          return queue
      self.input_queue = patch_queue_put_nowait(self.input_queue)

  - module: "vllm.v1.engine.core:EngineCoreProc"
    function_name: process_output_socket
    entry_operation: |
      import os, time
      import queue
      def patch_queue_get(queue):
          original_get = queue.get
          def patch_get():
              value = original_get()
              for request in value.outputs:
                  if os.getenv('ROLE') == "prefill":
                      print(f"<<<Action: Start to send output in prefill stage; Timestamp:{time.time()}; RequestID:{request.request_id}; Role:{os.getenv('ROLE')}")
              return value
          queue.get = patch_get
          return queue
      self.output_queue = patch_queue_get(self.output_queue)

## first token, second token
  - module: "vllm.v1.engine.output_processor:RequestOutputCollector"
    function_name: get_nowait
    exit_operation: |
      import os, time
      if result and (not os.getenv("ROLE") == "prefill"):
          if result.request_id in self.token_number_dict:
              self.token_number_dict[result.request_id] += 1
              if self.token_number_dict[result.request_id] == 2:
                  print(f"<<<Action: Secend decode output token; Timestamp:{time.time()}; RequestID:{result.request_id}; Role:{os.getenv('ROLE')}")
          else:
              self.token_number_dict[result.request_id] = 1
              print(f"<<<Action: First decode output token; Timestamp:{time.time()}; RequestID:{result.request_id}; Role:{os.getenv('ROLE')}")

  - module: "vllm.v1.engine.output_processor:RequestOutputCollector"
    function_name: get
    exit_operation: |
      import os, time
      if result and (not os.getenv("ROLE") == "prefill") :
          if result.request_id in self.token_number_dict:
              self.token_number_dict[result.request_id] += 1
              if self.token_number_dict[result.request_id] == 2:
                  print(f"<<<Action: Secend decode output token; Timestamp:{time.time()}; RequestID:{result.request_id}; Role:{os.getenv('ROLE')}")
          else:
              self.token_number_dict[result.request_id] = 1
              print(f"<<<Action: First decode output token; Timestamp:{time.time()}; RequestID:{result.request_id}; Role:{os.getenv('ROLE')}")
