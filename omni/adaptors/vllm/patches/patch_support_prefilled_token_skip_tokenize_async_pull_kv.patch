diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index f96233545..b6e722b37 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -45,6 +45,9 @@ from vllm.transformers_utils.tokenizers import (maybe_serialize_tool_calls,
 
 logger = init_logger(__name__)
 
+import os
+reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "1") == "1"
+skip_decode_tokenize = os.getenv("OMNI_SKIP_DECODE_TOKENIZE", "1") == "1"
 
 class OpenAIServingChat(OpenAIServing):
 
@@ -211,6 +214,14 @@ class OpenAIServingChat(OpenAIServing):
         try:
             for i, engine_prompt in enumerate(engine_prompts):
                 sampling_params: Union[SamplingParams, BeamSearchParams]
+                if reuse_prefilled_tokens:
+                    if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                        engine_prompt["prefilled_token_ids"] = request.kv_transfer_params["prefilled_token"]
+                        new_tokens = tokenizer.convert_ids_to_tokens(engine_prompt["prefilled_token_ids"][0])
+                        delta_text = tokenizer.convert_tokens_to_string([new_tokens])
+                        engine_prompt["prefilled_texts"] = delta_text
+                    else:
+                        raise ValueError("kv_transfer_params must contain 'prompt_token_ids' for operator_opt_config.reuse_prefilled_token.")
                 default_max_tokens = self.max_model_len - len(
                     engine_prompt["prompt_token_ids"])
                 if request.use_beam_search:
@@ -916,6 +927,22 @@ class OpenAIServingChat(OpenAIServing):
 
         assert final_res is not None
 
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                prompt_token_ids = request.kv_transfer_params["prefilled_token"]
+                new_tokens = tokenizer.convert_ids_to_tokens(prompt_token_ids[0])
+                prompt_text = tokenizer.convert_tokens_to_string([new_tokens])
+                final_res.outputs[0].text = prompt_text + final_res.outputs[0].text
+            else:
+                raise ValueError("kv_transfer_params must contain 'prefilled_token' for operator_opt_config.reuse_prefilled_tokens.")
+        ## In Prefill node, the response will carry prompt_token_ids with kv_transfer_params
+        if skip_decode_tokenize:
+            if final_res.kv_transfer_params:
+                final_res.kv_transfer_params["prompt_token_ids"] = final_res.prompt_token_ids
+        if reuse_prefilled_tokens:
+            if final_res.kv_transfer_params:
+                final_res.kv_transfer_params["prefilled_token"] = [final_res.outputs[0].token_ids[0]]
+
         choices: list[ChatCompletionResponseChoice] = []
 
         role = self.get_chat_request_role(request)
@@ -1073,6 +1100,11 @@ class OpenAIServingChat(OpenAIServing):
             num_prompt_tokens += len(final_res.encoder_prompt_token_ids)
         num_generated_tokens = sum(
             len(output.token_ids) for output in final_res.outputs)
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                num_generated_tokens += 1
+            else:
+                raise ValueError("kv_transfer_params must contain 'prefilled_token' for operator_opt_config.reuse_prefilled_tokens.")
         usage = UsageInfo(prompt_tokens=num_prompt_tokens,
                           completion_tokens=num_generated_tokens,
                           total_tokens=num_prompt_tokens +
diff --git a/vllm/entrypoints/openai/serving_engine.py b/vllm/entrypoints/openai/serving_engine.py
index 8b3857fad..4500fbded 100644
--- a/vllm/entrypoints/openai/serving_engine.py
+++ b/vllm/entrypoints/openai/serving_engine.py
@@ -75,6 +75,8 @@ from vllm.transformers_utils.tokenizer import AnyTokenizer, MistralTokenizer, ge
 from vllm.utils import (is_list_of, make_async, merge_async_iterators,
                         random_uuid, P, T)
 
+import os; skip_decode_tokenize = os.getenv("OMNI_SKIP_DECODE_TOKENIZE", "1") == "1"
+
 logger = init_logger(__name__)
 
 CompletionLikeRequest = Union[CompletionRequest, DetokenizeRequest,
@@ -1009,24 +1011,32 @@ class OpenAIServing:
             request = tool_parser(tokenizer).adjust_request(  # type: ignore
                 request=request)
 
-        if isinstance(request_prompt, str):
-            prompt_inputs = await self._tokenize_prompt_input_async(
-                request,
-                tokenizer,
-                request_prompt,
-                truncate_prompt_tokens=truncate_prompt_tokens,
-                add_special_tokens=add_special_tokens,
-            )
+        if skip_decode_tokenize:
+            if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                engine_prompt = TokensPrompt(
+                    prompt_token_ids=request.kv_transfer_params["prompt_token_ids"])
+            else:
+                raise ValueError("kv_transfer_params must contain 'prompt_token_ids' for operator_opt_config.skip_decode_tokenize.") 
         else:
-            # For MistralTokenizer
-            assert is_list_of(request_prompt, int), (
-                "Prompt has to be either a string or a list of token ids")
-            prompt_inputs = TextTokensPrompt(
-                prompt=tokenizer.decode(request_prompt),
-                prompt_token_ids=request_prompt)
-
-        engine_prompt = TokensPrompt(
-            prompt_token_ids=prompt_inputs["prompt_token_ids"])
+            if isinstance(request_prompt, str):
+                prompt_inputs = await self._tokenize_prompt_input_async(
+                    request,
+                    tokenizer,
+                    request_prompt,
+                    truncate_prompt_tokens=truncate_prompt_tokens,
+                    add_special_tokens=add_special_tokens,
+                )
+            else:
+                # For MistralTokenizer
+                assert is_list_of(request_prompt, int), (
+                    "Prompt has to be either a string or a list of token ids")
+                prompt_inputs = TextTokensPrompt(
+                    prompt=tokenizer.decode(request_prompt),
+                    prompt_token_ids=request_prompt)
+
+            engine_prompt = TokensPrompt(
+                prompt_token_ids=prompt_inputs["prompt_token_ids"])
+
         if mm_data is not None:
             engine_prompt["multi_modal_data"] = mm_data
         if request.mm_processor_kwargs is not None:
diff --git a/vllm/inputs/data.py b/vllm/inputs/data.py
index c83ab73b6..c87891fbb 100644
--- a/vllm/inputs/data.py
+++ b/vllm/inputs/data.py
@@ -8,6 +8,7 @@ from typing_extensions import NotRequired, TypedDict, TypeVar
 if TYPE_CHECKING:
     from vllm.multimodal.inputs import MultiModalDataDict, MultiModalInputs
 
+import os; reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "1") == "1"
 
 class TextPrompt(TypedDict):
     """Schema for a text prompt."""
@@ -62,7 +63,12 @@ class TokensPrompt(TypedDict):
     """
     Optional cache salt to be used for prefix caching.
     """
-
+    if reuse_prefilled_tokens:
+        """
+        This is used when the model supports reusing prefilled tokens.
+        """
+        prefilled_token_ids: Optional[list[int]] = []
+        prefilled_texts: Optional[str] = ""
 
 class EmbedsPrompt(TypedDict):
     """Schema for a prompt provided via token embeddings."""
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 722d85d3b..fe463dfdb 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -30,6 +30,7 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+import os; reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "1") == "1"
 
 logger = init_logger(__name__)
 
@@ -979,6 +980,13 @@ class Scheduler(SchedulerInterface):
         if num_computed_tokens == request.num_tokens:
             num_computed_tokens -= 1
 
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prefilled_token" in request.kv_transfer_params:
+                request.prompt_token_ids.extend(request.kv_transfer_params["prefilled_token"])
+                request.append_output_token_ids(request.kv_transfer_params["prefilled_token"])
+            else:
+                raise ValueError("kv_transfer_params must contain 'prefilled_token' for operator_opt_config.reuse_prefilled_tokens.")
+
         # with spec
         if self.vllm_config.speculative_config is not None:
             request.spec_token_ids.append(0)
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 5628f0a11..2a635ae90 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -16,7 +16,7 @@ from vllm.inputs.preprocess import InputPreprocessor
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry
-from vllm.outputs import RequestOutput
+from vllm.outputs import RequestOutput, CompletionOutput
 from vllm.pooling_params import PoolingParams
 from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
@@ -36,6 +36,8 @@ from vllm.v1.metrics.loggers import (StatLoggerBase, StatLoggerFactory,
                                      setup_default_loggers)
 from vllm.v1.metrics.stats import IterationStats, SchedulerStats
 
+import os; reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "1") == "1"
+
 logger = init_logger(__name__)
 
 
@@ -287,6 +289,30 @@ class AsyncLLM(EngineClient):
         """
 
         try:
+            if reuse_prefilled_tokens:
+                if "prefilled_token_ids" in prompt and prompt["prefilled_token_ids"] != []:
+                    if sampling_params.n == 1:
+                        output = RequestOutput(request_id=request_id,
+                                prompt=None, finished=False, prompt_logprobs=None,
+                                prompt_token_ids=prompt["prompt_token_ids"],
+                                outputs=[CompletionOutput(index=0,
+                                    cumulative_logprob=None, logprobs=None,
+                                    text= prompt["prefilled_texts"],
+                                    token_ids=prompt["prefilled_token_ids"])])
+                    else:
+                        # Fan out child requests (for n>1).
+                        parent_request = ParentRequest(request_id, sampling_params)
+                        for idx in range(sampling_params.n):
+                            request_id_child, params = parent_request.get_child_info(idx)
+                            output = RequestOutput(request_id=request_id_child,
+                                    prompt=None, finished=False, prompt_logprobs=None,
+                                    prompt_token_ids=prompt["prompt_token_ids"],
+                                    outputs=[CompletionOutput(index=idx,
+                                        cumulative_logprob=None, logprobs=None,
+                                        text= prompt["prefilled_texts"],
+                                        token_ids=prompt["prefilled_token_ids"])])
+                    prompt["prefilled_token_ids"] = []
+                    yield output
             # We start the output_handler on the first call to generate() so
             # we can call __init__ before the event loop, which enables us
             # to handle startup failure gracefully in the OpenAI server.
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 4d5f4d3d1..56018965b 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -39,6 +39,10 @@ from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
 from vllm.v1.structured_output import StructuredOutputManager
 from vllm.version import __version__ as VLLM_VERSION
 
+import os; async_pull_kv = os.getenv("OMNI_ASYNC_PULL_KV", "1") == "1"
+if async_pull_kv :
+    from vllm.v1.core.kv_cache_manager import KVCacheBlocks, KVCacheManager
+
 logger = init_logger(__name__)
 
 POLLING_TIMEOUT_S = 2.5
@@ -62,6 +66,8 @@ class EngineCore:
                     VLLM_VERSION, vllm_config)
 
         self.log_stats = log_stats
+        if async_pull_kv :
+            self.pull_kv_lock = threading.Lock()
 
         # Setup Model.
         self.model_executor = executor_class(vllm_config)
@@ -182,10 +188,13 @@ class EngineCore:
             # Start grammar compilation asynchronously
             self.structured_output_manager.grammar_init(req)
 
-        if req.kv_transfer_params is not None and (
-                not self.scheduler.get_kv_connector()):
-            logger.warning("Got kv_transfer_params, but no KVConnector found. "
-                           "Disabling KVTransfer for this request.")
+        if req.kv_transfer_params is not None:
+            if not self.scheduler.get_kv_connector():
+                logger.warning("Got kv_transfer_params, but no KVConnector found. "
+                            "Disabling KVTransfer for this request.")
+            if async_pull_kv :
+                if req.kv_transfer_params["fast_path"]:
+                    req.status = RequestStatus.WAITING_FOR_REMOTE_KVS
 
         self.scheduler.add_request(req)
 
@@ -218,7 +227,11 @@ class EngineCore:
                 outputs=[],
                 scheduler_stats=self.scheduler.make_stats(),
             )
-        scheduler_output = self.scheduler.schedule()
+        if async_pull_kv :
+            with self.pull_kv_lock:
+                scheduler_output = self.scheduler.schedule()
+        else:
+            scheduler_output = self.scheduler.schedule()
         model_output = self.execute_model(scheduler_output)
         engine_core_outputs = self.scheduler.update_from_output(
             scheduler_output, model_output)  # type: ignore
@@ -408,6 +421,7 @@ class EngineCoreProc(EngineCore):
                 args=(output_address, engine_index),
                 daemon=True)
             self.output_thread.start()
+            set_thread_affinity(self.output_thread, 51)  # Set affinity to CPU 51
         finally:
             if input_socket is not None:
                 input_socket.close(linger=0)
@@ -606,9 +620,50 @@ class EngineCoreProc(EngineCore):
                 request_type == EngineCoreRequestType.ADD) else generic_decoder
             request = decoder.decode(data_frames)
 
+            if async_pull_kv :
+                if request_type == EngineCoreRequestType.ADD:
+                    with self.pull_kv_lock:
+                        self.try_pull_kv_fast_path(request)
+
             # Push to input queue for core busy loop.
             self.input_queue.put_nowait((request_type, request))
 
+    def try_pull_kv_fast_path(self, request: EngineCoreRequest):
+        req = Request.from_engine_core_request(request)
+        
+        if req.kv_transfer_params is not None:
+            req.kv_transfer_params["fast_path"] = False
+
+            connector = self.scheduler.connector
+            num_external_computed_tokens, load_kv_async = (
+                    (0, False) if connector is None else
+                    connector.get_num_new_matched_tokens(
+                        req, 0))
+
+            new_computed_blocks = KVCacheBlocks.create_empty()
+
+            new_blocks = self.scheduler.kv_cache_manager.allocate_slots(
+                    req,
+                    num_external_computed_tokens,
+                    0,
+                    new_computed_blocks,
+                    num_lookahead_tokens=0,
+                    delay_cache_blocks=load_kv_async,
+                )
+            if new_blocks is None:
+                # Let's delay to slow path.
+                return
+
+            req.kv_transfer_params["fast_path"] = True
+            connector.update_state_after_alloc(
+                        req,
+                        new_blocks,
+                        num_external_computed_tokens,
+                    )
+           
+            # Kick off pulling kv directly to worker thread
+            connector.build_connector_meta(None)
+
     def process_output_socket(self, output_path: str, engine_index: int):
         """Output socket IO thread."""
 
@@ -791,3 +846,11 @@ class DPEngineCoreProc(EngineCoreProc):
 
         return ParallelConfig.has_unfinished_dp(self.dp_group,
                                                 local_unfinished)
+
+def set_thread_affinity(thread, cpu_id):
+    import threading
+    import ctypes
+    while not hasattr(thread, "native_id"):
+        pass
+    tid = thread.native_id
+    os.sched_setaffinity(tid, {cpu_id})
\ No newline at end of file
