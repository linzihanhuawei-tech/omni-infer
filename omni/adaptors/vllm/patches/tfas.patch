From 86970e947cbd54f1c81c5f320a8edadf7656f5c0 Mon Sep 17 00:00:00 2001
From: zhangwei <mathzhangwei@gmailcom>
Date: Tue, 22 Jul 2025 17:30:57 +0800
Subject: [PATCH] tfas scheduler patch

---
 vllm/v1/core/sched/scheduler.py | 71 ++++++++++++++++++++++++++++++++-
 1 file changed, 70 insertions(+), 1 deletion(-)

diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index b83489f6d..121c12837 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -30,12 +30,34 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+from omni.adaptors.vllm import envs as envs_omni
+import math
 
 logger = init_logger(__name__)
 
 import os
 reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
 
+def tfas_compute_upper_bound(waiting_queue:list[Request]):
+    req_in_waiting_queue = 0
+    for request in waiting_queue:
+        req_in_waiting_queue += request.num_prompt_tokens
+    req_in_waiting_queue = req_in_waiting_queue / 1024
+    return int(
+        math.sqrt(
+            req_in_waiting_queue
+            * envs_omni.TFAS_INTERCEPT
+            / envs_omni.TFAS_SLOPE
+        ) * 1024
+    )
+
+
+def tfas_length_sort_time_decay(now_time, request:Request):
+    if now_time - request.arrival_time > envs_omni.TFAS_WAITING_TIME_OUT:
+        return 0
+    else:
+        return request.num_prompt_tokens
+
 class Scheduler(SchedulerInterface):
 
     def __init__(
@@ -80,9 +102,19 @@ class Scheduler(SchedulerInterface):
         # will have a corresponding KVConnector with Role=WORKER.
         # KV Connector pushes/pull of remote KVs for P/D and offloading.
         self.connector = None
+        self.enable_tfas = False
+        self.enable_tfas_profiler = False
         if self.vllm_config.kv_transfer_config is not None:
             self.connector = KVConnectorFactory.create_connector_v1(
                 config=self.vllm_config, role=KVConnectorRole.SCHEDULER)
+            if (self.vllm_config.kv_transfer_config.is_kv_producer and 
+                envs_omni.PREFILL_SCHEDULE_POLICY=='tfas'):
+                logger.info("[scheduler.py] TFAS policy is enabled")
+                self.enable_tfas = True
+            if (self.vllm_config.kv_transfer_config.is_kv_producer and 
+                envs_omni.PREFILL_SCHEDULE_POLICY=='tfas_profiler'):
+                logger.info("[scheduler.py] TFAS_profiler policy is enabled")
+                self.enable_tfas_profiler = True
 
         self.kv_event_publisher = EventPublisherFactory.create(
             self.kv_events_config)
@@ -188,13 +220,41 @@ class Scheduler(SchedulerInterface):
         encoder_budget = self.max_num_encoder_input_tokens
         # Spec decode-related.
         scheduled_spec_decode_tokens: dict[str, list[int]] = {}
-
+            
+        if self.enable_tfas:
+            now_time = time.time()
+            self.waiting = deque(
+                sorted(
+                    self.waiting,
+                    key=lambda seq_group: tfas_length_sort_time_decay(
+                        now_time, seq_group)
+                )
+            )
+            upper_bound = tfas_compute_upper_bound(self.waiting)
+            tfas_token_budget = min(
+                max(envs_omni.TFAS_REAL_TOKEN_BUDGET, upper_bound), 
+                self.max_num_scheduled_tokens)
+
+        if self.enable_tfas_profiler:
+            if hasattr(self, "trigger_num"):
+                self.trigger_num += 1
+            else:
+                self.trigger_num = 1
+                self.grow_frequency = 30
+                self.grow_upper_bound = self.max_num_running_reqs 
+            tfas_profiler_max_num_running_reqs = (
+                self.trigger_num//self.grow_frequency+1)
+            tfas_profiler_max_num_running_reqs = min(self.tfas_profiler_max_num_running_reqs,
+                                                      self.grow_upper_bound)
+            
         # For logging.
         scheduled_timestamp = time.monotonic()
 
         # First, schedule the RUNNING requests.
         req_index = 0
         while req_index < len(self.running) and token_budget > 0:
+            if self.enable_tfas and tfas_token_budget <= 0:
+                break
             request = self.running[req_index]
 
             num_new_tokens = (request.num_tokens_with_spec -
@@ -285,6 +345,8 @@ class Scheduler(SchedulerInterface):
                 new_blocks.get_block_ids())
             num_scheduled_tokens[request.request_id] = num_new_tokens
             token_budget -= num_new_tokens
+            if self.enable_tfas:
+                tfas_token_budget -= num_new_tokens
             req_index += 1
 
             # Speculative decode related.
@@ -322,6 +384,11 @@ class Scheduler(SchedulerInterface):
         # Next, schedule the WAITING requests.
         if not preempted_reqs:
             while self.waiting and token_budget > 0:
+                if self.enable_tfas and tfas_token_budget <= 0:
+                    break
+                if (self.enable_tfas_profiler and 
+                    len(self.running) == tfas_profiler_max_num_running_reqs):
+                    break
                 if len(self.running) == self.max_num_running_reqs:
                     break
 
@@ -475,6 +542,8 @@ class Scheduler(SchedulerInterface):
                     self.kv_cache_manager.get_block_ids(request.request_id))
                 num_scheduled_tokens[request.request_id] = num_new_tokens
                 token_budget -= num_new_tokens
+                if self.enable_tfas:
+                    tfas_token_budget -= num_new_tokens
                 request.status = RequestStatus.RUNNING
                 request.num_computed_tokens = num_computed_tokens
 
-- 
2.43.0

From 8893c04aa8f726e4944c5b93f049db587c5f52ee Mon Sep 17 00:00:00 2001
From: zhangwei <mathzhangwei@gmailcom>
Date: Tue, 22 Jul 2025 17:39:53 +0800
Subject: [PATCH] tfas api_server patch

---
 vllm/entrypoints/openai/api_server.py | 12 ++++++++++++
 1 file changed, 12 insertions(+)

diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py
index e9192e9ba..65b3135e7 100644
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -100,6 +100,7 @@ from vllm.usage.usage_lib import UsageContext
 from vllm.utils import (Device, FlexibleArgumentParser, get_open_zmq_ipc_path,
                         is_valid_ipv6_address, set_ulimit)
 from vllm.version import __version__ as VLLM_VERSION
+from omni.adaptors.vllm import envs as envs_omni
 
 TIMEOUT_KEEP_ALIVE = 120  # seconds
 
@@ -110,6 +111,16 @@ logger = init_logger('vllm.entrypoints.openai.api_server')
 
 _running_tasks: set[asyncio.Task] = set()
 
+def adapt_args_tfas(args):
+    # tfas adjusts the batch size dynamically according to the throughput saturation point.
+    # Therefore, the upper limit should be set sufficiently high to prevent overflow or constraint violations.
+    if (hasattr(args, "max_num_seqs") and 
+        getattr(args, "kv_transfer_config", None) and
+        args.kv_transfer_config.kv_role == "kv_producer" and 
+        envs_omni.PREFILL_SCHEDULE_POLICY in ["tfas", "tfas_profiler"]):
+        logger.info("[api_server.py] TFAS or TFAS_Profiler policy enabled; max_num_seqs set to 128.")
+        tfas_max_num_seqs = 128
+        args.max_num_seqs = max(tfas_max_num_seqs, 4*args.max_num_seqs)
 
 @asynccontextmanager
 async def lifespan(app: FastAPI):
@@ -1287,6 +1298,7 @@ def create_server_socket(addr: tuple[str, int]) -> socket.socket:
 
 async def run_server(args, **uvicorn_kwargs) -> None:
     logger.info("vLLM API server version %s", VLLM_VERSION)
+    adapt_args_tfas(args)
     log_non_default_args(args)
 
     if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:
-- 
2.43.0

From b91b1c45a188f232b72e0fbde943c5128ec1ebf7 Mon Sep 17 00:00:00 2001
From: zhangwei <mathzhangwei@gmailcom>
Date: Fri, 18 Jul 2025 14:40:36 +0800
Subject: [PATCH 1/3] Patch request.py to support TFAS

---
 vllm/v1/request.py | 1 +
 1 file changed, 1 insertion(+)

diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index d2843b65a..8ad5496c9 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -31,6 +31,7 @@ class Request:
         structured_output_request: Optional["StructuredOutputRequest"] = None,
         cache_salt: Optional[str] = None,
     ) -> None:
+        self.arrival_time = arrival_time
         self.request_id = request_id
         self.sampling_params = sampling_params
         # Because of LoRA, the eos token id can be different for each request.
-- 
2.43.0

