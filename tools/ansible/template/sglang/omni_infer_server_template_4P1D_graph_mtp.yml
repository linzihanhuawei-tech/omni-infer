# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

- name: run sglang
  hosts: all
  any_errors_fatal: true
  max_fail_percentage: 0
  gather_facts: yes

  environment:
    # Global Configuration 
    LOG_PATH: "/data/log_path"
    MODEL_PATH: "/data/models/DeepSeek-R1-w8a8-fusion"
    LOG_PATH_IN_EXECUTOR: "/data/log_path_in_executor"
    CODE_PATH: "/data/local_code_path"

    # Configuration for containers 
    DOCKER_IMAGE_ID: "REPOSITORY:TAG"
    DOCKER_NAME_P: "sglang_prefill_container"
    DOCKER_NAME_D: "sglang_decode_container"
    DOCKER_NAME_C: "sglang_proxy_container"
    SCRIPTS_PATH: "/tmp/scripts_path"
    GLOO_SOCKET_IFNAME: "network_interface"

  vars:
    
    docker_run_cmd: |
      docker run -it --shm-size=500g \
        -e CODE_PATH=$CODE_PATH \
        -e LOG_PATH=$LOG_PATH \
        --net=host \
        --privileged=true \
        -u root \
        -w /data \
        --device=/dev/davinci0:/dev/davinci0 --device=/dev/davinci1:/dev/davinci1 \
        --device=/dev/davinci2:/dev/davinci2 --device=/dev/davinci3:/dev/davinci3 \
        --device=/dev/davinci4:/dev/davinci4 --device=/dev/davinci5:/dev/davinci5 \
        --device=/dev/davinci6:/dev/davinci6 --device=/dev/davinci7:/dev/davinci7 \
        --device=/dev/davinci8:/dev/davinci8 --device=/dev/davinci9:/dev/davinci9 \
        --device=/dev/davinci10:/dev/davinci10 --device=/dev/davinci11:/dev/davinci11 \
        --device=/dev/davinci12:/dev/davinci12 --device=/dev/davinci13:/dev/davinci13 \
        --device=/dev/davinci14:/dev/davinci14 --device=/dev/davinci15:/dev/davinci15 \
        --device=/dev/davinci_manager \
        --device=/dev/hisi_hdc \
        --device=/dev/devmm_svm \
        --entrypoint=bash \
        -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
        -v /usr/local/dcmi:/usr/local/dcmi \
        -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
        -v /etc/ascend_install.info:/etc/ascend_install.info \
        -v /usr/local/sbin:/usr/local/sbin \
        -v /etc/hccn.conf:/etc/hccn.conf \
        -v /usr/bin/hccn_tool:/usr/bin/hccn_tool \
        -v $LOG_PATH:$LOG_PATH \
        -v $MODEL_PATH:$MODEL_PATH \
        -v /data:/data \
        -v $SCRIPTS_PATH:$SCRIPTS_PATH \
        -v $CODE_PATH:$CODE_PATH \
        -v /dev/shm:/dev/shm \
        -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \

    docker_exec_cmd: |
      docker exec \

    run_sglang_server_prefill_cmd: |
      #!/bin/bash

      . ~/.bashrc
      export HCCL_INTRA_ROCE_ENABLE=1
      export HCCL_DETERMINISTIC=false
      export HCCL_OP_EXPANSION_MODE="AIV"
      export GLOO_SOCKET_IFNAME={{ansible_env.GLOO_SOCKET_IFNAME}}
      export MLP_TP_SIZE=4
      export PYTHONPATH=${CODE_PATH}/omniinfer/infer_engines/sglang/python:${CODE_PATH}/omniinfer:$PYTHONPATH
      export IS_PREFILL=1

      rm -rf .torchair_cache/
      python3 -m sglang.launch_server \
        --model-path ${MODEL_PATH} \
        --device npu \
        --trust-remote-code \
        --attention-backend npumla \
        --moe-a2a-backend deepep \
        --deepep-mode normal \
        --quantization w8a8_int8 \
        --disaggregation-mode prefill \
        --host $HOST_IP \
        --port $API_PORT \
        --trust-remote-code \
        --tp-size 16 \
        --mem-fraction-static 0.90 \
        --disaggregation-transfer-backend mooncake \
        --disable-radix-cache \
        --max-prefill-tokens 30000 \
        --max-total-tokens 30000 \
        --speculative-algorithm MTP \
        --speculative-num-steps 1 \
        --speculative-num-draft-tokens 1 \
        --speculative-eagle-topk 1 \
        --chunked-prefill-size -128 \
        --max-running-requests 32 > ${LOG_PATH}/run_prefill_sglang.log 2>&1 &

    run_sglang_server_decode_cmd: |
      #!/bin/bash

      . ~/.bashrc

      export HCCL_INTRA_ROCE_ENABLE=1
      export HCCL_DETERMINISTIC=false
      export HCCL_OP_EXPANSION_MODE="AIV"
      export HCCL_BUFFSIZE=500
      export GLOO_SOCKET_IFNAME={{ansible_env.GLOO_SOCKET_IFNAME}}
      export PYTHONPATH=${CODE_PATH}/omniinfer/infer_engines/sglang/python:${CODE_PATH}/omniinfer:$PYTHONPATH
      export MLP_TP_SIZE=4
      dp=$(echo -n "$DECODE_DATA_PARALLEL_SIZE" | tr -cd ',' | wc -c)
      ((dp++))

      rm -rf .torchair_cache/
      python3 ${CODE_PATH}/omniinfer/tools/scripts/start_api_servers_sgl.py \
        --model-path ${MODEL_PATH} \
        --device npu \
        --trust-remote-code \
        --attention-backend npumla \
        --moe-a2a-backend deepep \
        --deepep-mode normal \
        --enable-dp-attention \
        --quantization w8a8_int8 \
        --disaggregation-mode decode \
        --host $IP \
        --port $API_PORT \
        --trust-remote-code \
        --tp-size ${dp} \
        --dp-size ${dp} \
        --nnodes $NNODES \
        --node-rank $NODE_RANK \
        --mem-fraction-static 0.85 \
        --disaggregation-transfer-backend mooncake \
        --disable-radix-cache \
        --dist-init-addr $HOST_IP:16732 \
        --skip-server-warmup \
        --max-prefill-tokens 30000 \
        --max-total-tokens 204800 \
        --speculative-algorithm MTP \
        --speculative-num-steps 1 \
        --speculative-num-draft-tokens 1 \
        --speculative-eagle-topk 1 \
        --enable-torch-compile \
        --cuda-graph-bs 64 \
        --torch-compile-max-bs 64 \
        --ep-size ${dp} \
        --pp-size 1 \
        --enable-multi-api-server \
        --max-running-requests 2048 > ${LOG_PATH}/run_decode_sglang.log 2>&1 &

    run_proxy_cmd: |
      #!/bin/bash

      prefill_result="{{ PREFILL_API_SERVER_LIST }}"
      prefill_result=`echo "$prefill_result" | awk '$1=$1'`

      decode_result=""
      decode_api_servers="{{ DECODE_API_SERVER_LIST }}"
      decode_api_servers=`echo "$decode_api_servers" | awk '$1=$1'`
      decode_array=(${decode_api_servers//,/ })
      for var in ${decode_array[@]}; do
        address=${var%@*}
        ip=${address%:*}
        port=${address#*:}
        num=${var#*@}
        for ((i=0; i<=$num;i++)); do
          if [[ -z ${decode_result} ]]; then
            decode_result="$ip:$port"
          else
            decode_result="${decode_result},$ip:$port"
          fi
          ((port++))
        done
      done

      export http_proxy="http://p_atlas:proxy%40123@172.18.100.92:8080"
      export https_proxy="http://p_atlas:proxy%40123@172.18.100.92:8080"
      export GIT_SSL_NO_VERIFY=true
      mkdir /tmp/omnipath
      cd /tmp/omnipath
      git config --global http.sslVerify "false"
      # git clone https://gitee.com/wuhang1991/omniinfer.git -b forsglang
      git clone -b gproxy_sgl https://gitee.com/august_ust/omniinfer_sgl.git 
      mv /tmp/omnipath/omniinfer_sgl /tmp/omnipath/omniinfer
      cd /tmp/omnipath/omniinfer/omni/accelerators/sched/global_proxy

      unset http_proxy
      unset https_proxy
      export http_proxy="http://p_atlas:proxy%40123@172.18.100.92:8080"
      export https_proxy="http://p_atlas:proxy%40123@172.18.100.92:8080"
      export no_proxy=127.0.0.1,.huawei.com,localhost,local,.local

      NGINX_VERSION="${NGINX_VERSION:-1.24.0}"
      if [ ! -f "nginx-${NGINX_VERSION}.tar.gz" ]; then
        wget --no-check-certificate "https://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz" > ${LOG_PATH}/install_nginx.log 2>&1
      fi

      tar -zxf "nginx-${NGINX_VERSION}.tar.gz"
      
      rm  -ivf /etc/yum.repos.d/*

      echo "[openEuler-everything]
      name=openEuler-everything
      baseurl=http://mirrors.tools.huawei.com/openeuler/openEuler-22.03-LTS-SP4/everything/aarch64/
      enabled=1
      gpgcheck=0
      gpgkey=http://mirrors.tools.huawei.com/openeuler/openEuler-22.03-LTS-SP4/everything/aarch64/RPM-GPG-KEY-openEuler
      [openEuler-EPOL]
      name=openEuler-epol
      baseurl=http://mirrors.tools.huawei.com/openeuler/openEuler-22.03-LTS-SP4/EPOL/main/aarch64/
      enabled=1
      gpgcheck=0
      [openEuler-update]
      name=openEuler-update
      baseurl=http://mirrors.tools.huawei.com/openeuler/openEuler-22.03-LTS-SP4/update/aarch64/
      enabled=1
      gpgcheck=0" > /etc/yum.repos.d/openeuler.repo

      unset http_proxy
      unset https_proxy
      bash build.sh >> ${LOG_PATH}/install_nginx.log 2>&1

      bash global_proxy.sh \
        --listen-port "$PROXY_NODE_PORT" \
        --prefill-servers-list "$prefill_result" \
        --decode-servers-list "$decode_result" \
        --log-file ${LOG_PATH}/nginx_error.log \
        --log-level error \
        --core-num 4 \
        --start-core-index 0 \
        --engine-type sglang \
        --bootstrap-port 8998 >> ${LOG_PATH}/nginx_error.log 2>&1

    kill_python_processes_cmd: |
      #!/bin/bash

      ps aux | grep "python" | grep -v "grep" | awk '{print $2}' | xargs kill -9
      ps aux | grep "python3" | grep -v "grep" | awk '{print $2}' | xargs kill -9
      ps aux | grep "sglang" | grep -v "grep" | awk '{print $2}' | xargs kill -9

    kill_nginx_processes_cmd: |
      #!/bin/bash

      ps aux | grep "nginx" | grep -v "grep" | awk '{print $2}' | xargs kill -9

    start_docker_cmd_p: >
      {{ docker_run_cmd }}
      -d --name $DOCKER_NAME_P $DOCKER_IMAGE_ID

    start_docker_cmd_d: >
      {{ docker_run_cmd }} 
      -d --name $DOCKER_NAME_D $DOCKER_IMAGE_ID

    start_docker_cmd_c: >
      {{ docker_run_cmd }} 
      -e PROXY_NODE_PORT=$NODE_PORT
      -d --name $DOCKER_NAME_C $DOCKER_IMAGE_ID

    docker_start_sglang_cmd_p: >
      {{ docker_exec_cmd }}
      -e MODEL_PATH=$MODEL_PATH
      -e API_PORT=$API_PORT
      -e NODE_RANK=$NODE_RANK
      -e HOST_IP=$HOST_IP
      -d $DOCKER_NAME_P 
      /bin/bash -c {{ansible_env.SCRIPTS_PATH}}/sglang_run_for_p.sh

    docker_start_sglang_cmd_d: >
      {{ docker_exec_cmd }}
      -e MODEL_PATH=$MODEL_PATH
      -e DECODE_DATA_PARALLEL_SIZE=$DECODE_DATA_PARALLEL_SIZE
      -e API_PORT=$API_PORT
      -e HOST_IP=$HOST_IP
      -e NODE_RANK=$NODE_RANK
      -e IP=$IP
      -e NNODES=$NNODES
      -d $DOCKER_NAME_D 
      /bin/bash -c {{ansible_env.SCRIPTS_PATH}}/sglang_run_for_d.sh

    docker_start_proxy_cmd_c: >
      {{ docker_exec_cmd }}
      -e PREFILL_LB_SDK=$PREFILL_LB_SDK
      -e DECODE_LB_SDK=$DECODE_LB_SDK
      -d $DOCKER_NAME_C
      /bin/bash -c $SCRIPTS_PATH/run_proxy_server.sh

    docker_cp_prefill_code_cmd: "docker cp {{ ansible_env.CODE_PATH }}/omniinfer $DOCKER_NAME_P:/workspace/"
    docker_cp_decode_code_cmd: "docker cp {{ ansible_env.CODE_PATH }}/omniinfer $DOCKER_NAME_D:/workspace/"
    docker_update_prefill_code_cmd: "{{ docker_exec_cmd }} $DOCKER_NAME_P /bin/bash -c '. ~/.bashrc '"
    docker_update_decode_code_cmd: "{{ docker_exec_cmd }} $DOCKER_NAME_D /bin/bash -c '. ~/.bashrc '"

  tasks:
    - name: generate container name. #1、生成容器名称
      set_fact:
        ACTUAL_DOCKER_NAME_P: "{{ ansible_env.DOCKER_NAME_P }}_{{ inventory_hostname }}"
        ACTUAL_DOCKER_NAME_D: "{{ ansible_env.DOCKER_NAME_D }}_{{ inventory_hostname }}"
        ACTUAL_DOCKER_NAME_C: "{{ ansible_env.DOCKER_NAME_C }}_{{ inventory_hostname }}"
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      tags: 
        - always
      
    - name: Check and delete Prefill group Docker containers. #2、删除P节点上的同名容器
      block:
        - name: Check whether the container exists.  
          shell: |
            docker inspect --format='{{"{{.Name}}"}}' \
            $DOCKER_NAME_P \
            2>/dev/null | grep -v '^$'
          register: existing_containers
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          failed_when: false
          changed_when: false

        - name: Show generated container name. 
          debug:
            msg: "Generated container name: {{ existing_containers.stdout_lines }}"

        - name: Stop containers.  
          command: |
            /bin/bash -c "docker stop {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""

        - name: Delete containers.  
          command: |
            /bin/bash -c "docker rm -f {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""
      when: "'P' in group_names"
      tags: 
        - run_docker
        - clean_up
        - run_p

    - name: Check and delete Decode group Docker containers. #3、删除D节点上的同名容器
      block:
        - name: Check whether the container exists.  
          shell: |
            docker inspect --format='{{"{{.Name}}"}}' \
            $DOCKER_NAME_D \
            2>/dev/null | grep -v '^$'
          register: existing_containers
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          failed_when: false
          changed_when: false

        - name: Show generated container name. 
          debug:
            msg: "Generated container name: {{ existing_containers.stdout_lines }}"

        - name: Stop containers.  
          command: |
            /bin/bash -c "docker stop {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""

        - name: Delete containers.  
          command: |
            /bin/bash -c "docker rm -f {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""
      when: "'D' in group_names"
      tags: 
        - run_docker
        - clean_up
        - run_d

    - name: Check and delete containers used for global proxy server. #4、删除C节点上的同名容器
      block:
        - name: Check whether the container exists.  
          shell: |
            docker inspect --format='{{"{{.Name}}"}}' \
            $DOCKER_NAME_C \
            2>/dev/null | grep -v '^$'
          register: existing_containers
          environment:
            DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
          failed_when: false
          changed_when: false

        - name: Show generated container name. 
          debug:
            msg: "Generated container name: {{ existing_containers.stdout_lines }}"

        - name: Stop containers.  
          command: |
            /bin/bash -c "docker stop {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""

        - name: Delete containers.  
          command: |
            /bin/bash -c "docker rm -f {{ existing_containers.stdout_lines | join(' ') }}"
          when: existing_containers.stdout != ""
      when: "'C' in group_names"
      tags: 
        - run_docker
        - clean_up
        - run_c

    - name: Run container for prefill instances. #5、启动P节点容器：$DOCKER_NAME_P $DOCKER_IMAGE_ID
      command: bash -c "{{ start_docker_cmd_p }}"
      environment:
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
      when: "'P' in group_names"
      tags: 
        - run_docker
        - run_p
    
    - name: Run container for decode instances. #6、启动D节点容器：$DOCKER_NAME_D $DOCKER_IMAGE_ID
      command: bash -c "{{ start_docker_cmd_d }}"
      environment:
        DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
      when: "'D' in group_names"
      tags: 
        - run_docker
        - run_d

    - name: Run container for global proxy server. #7、启动C节点容器：$DOCKER_NAME_C $DOCKER_IMAGE_ID node_port
      command: bash -c "{{ start_docker_cmd_c }}"
      environment:
        NODE_PORT: "{{ node_port }}"
        DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
      when: "'C' in group_names"
      tags: 
        - run_docker
        - run_c

    - name: Use system commands to obtain the IP address of the executor. #8、在执行机上执行命令获取该机器的IP，并保存在ip_output变量中
      command: hostname -I
      register: ip_output
      run_once: yes
      delegate_to: localhost
      tags: 
        - sync_code
        - always

    - name: Extract the first valid IP address. #9、从ip_output中获取第一个有效ip，并保存在executor_ip
      set_fact:
        executor_ip: "{{ ip_output.stdout.split() | first }}"
      run_once: yes
      delegate_to: localhost
      tags: 
        - sync_code
        - always

    - name: Delete the old code on the target machine. #10、删除非执行机上CODE_PATH/sglang路径下的所有文件
      command: /bin/bash -c "rm -rf {{ ansible_env.CODE_PATH }}/*"
      when: hostvars[inventory_hostname].ansible_host != executor_ip
      tags:
        - sync_code
        - always

    - name: Delete the old log on the target machine. #11、删除所有机器上LOG_PATH路径下的所有文件
      command: /bin/bash -c "rm -rf {{ ansible_env.LOG_PATH }}/*"
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      tags:
        - sync_code
        - clean

    - name: Create a directory to store the code. #12、若不存在CODE_PATH目录，则创建
      ansible.builtin.file:
        path: "{{ ansible_env.CODE_PATH }}"
        state: directory
      tags: 
        - sync_code
        - always

    - name: The executor synchronizes code to all instances. #13、仅在非执行节点上，且属于 P/D 组或 C 组但不在 P/D 组中的主机上，将代码从执行节点同步到目标主机的 CODE_PATH 目录中
      synchronize:
        src: "{{ ansible_env.CODE_PATH }}/omniinfer"
        dest: "{{ ansible_env.CODE_PATH }}"
      when: >
        hostvars[inventory_hostname].ansible_host != executor_ip and 
        (
          'P' in group_names or
          'D' in group_names or
          (
            'C' in group_names and 
            ansible_host not in 
            (groups.get('P', []) | map('extract', hostvars, 'ansible_host') | list) and 
            ansible_host not in 
            (groups.get('D', []) | map('extract', hostvars, 'ansible_host') | list)
          )
        )
      tags: 
        - sync_code
        - always

    - name: Copy the code from the host machine into the container (prefill).
      command: bash -c "{{ docker_cp_prefill_code_cmd }}"
      environment: 
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
      when: "'P' in group_names"
      tags: install

    - name: Copy the code from the host machine into the container (decode).
      command: bash -c "{{ docker_cp_decode_code_cmd }}"
      environment: 
        DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
      when: "'D' in group_names"
      tags: install

    - name: docker_update_prefill_code_cmd.
      command: bash -c "{{ docker_update_prefill_code_cmd }}"
      environment:
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
      when: "'P' in group_names"
      tags: install

    - name: docker_update_decode_code_cmd.
      command: bash -c "{{ docker_update_decode_code_cmd }}"
      environment:
        DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
      when: "'D' in group_names"
      tags: install

    - name: Delete temporary script files. #14、删除P、D、C节点下SCRIPTS_PATH目录的所有文件
      command: /bin/bash -c "rm -rf ${SCRIPTS_PATH}/*"
      register: cmd_result
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      tags:
        - run_docker
        - clean_up
        - always

    - name: Register all values. 
      set_fact:  
        PREFILL_API_SERVER_LIST: >- #15.1、生成 Prefill 节点的 API 服务地址列表
          {% set result=[] %}
          {% for host in groups['P']|default([]) %}
            {% set h=hostvars.get(host,{}) %}
            {% set ansible_host_val=h.ansible_host|default('') %}
            {% set host_ip_val=h.host_ip|default('') %}
            {% set api_port_val=h.api_port|default('9000') %}
            {% if ansible_host_val and host_ip_val and ansible_host_val == host_ip_val %}
              {% set entry=ansible_host_val~':'~api_port_val %}
              {% if entry not in result %}
              {% set _=result.append(entry) %}
              {% endif %}
            {% endif %}
          {% endfor %}
          {{ result|join(',') }}
        DECODE_API_SERVER_LIST: >- #15.2、生成 Decode 节点的 API 服务地址列表
          {% set result=[] %}
          {% for host in groups['D']|default([]) %}
            {% set h=hostvars.get(host,{}) %}
            {% set ip=h.ansible_host|default('') %}
            {% set port=h.api_port|default('9100') %}
            {% set num=h.ascend_rt_visible_devices.count(',')|default('0') %}
            {% if ip %}
              {% set entry=ip~':'~port~'@'~num %}
              {% if entry not in result %}
                {% set _=result.append(entry) %}
              {% endif %}
            {% endif %}
          {% endfor %}
          {{ result | join(',') }}
        PREFILL_POD_NUM: >- #15.3、统计 Prefill 节点的唯一 IP 数量
          {{
            groups['P'] | 
            map('extract', hostvars) | 
            map(attribute='host_ip') | 
            unique | 
            length 
          }}
        DECODE_POD_NUM: "{{ groups['D'] | length }}" #15.4、统计 Decode 节点数量
        DECODE_SERVER_IP_LIST: >- #15.5、生成 Decode 节点的 IP 列表，按某种顺序排列
          {% set host_list = [] %}
          {% for host in groups['D'] %}
            {% if hostvars[host].host_ip == hostvars[host].ansible_host %}
              {% set _ = host_list.insert(0, hostvars[host].ansible_host) %}
            {% else %}
              {% set _ = host_list.append(hostvars[host].ansible_host) %}
            {% endif %}
          {% endfor %}         
          {{ host_list | join(',') }}
        DECODE_SERVER_ALL: "{{ groups['D'] | map('extract', hostvars) | map(attribute='ascend_rt_visible_devices') | select('defined') | join(',') }}" #15.6、获取 Decode 节点的 ascend_rt_visible_devices 设备信息
        DECODE_SERVER_OFFSET: "{% set offsets = {} %}{% set ns = namespace(cnt=0) %}{% for host in groups['D']|default([]) %}{% set _ = offsets.update({host: ns.cnt}) %}{% set num=hostvars[host].ascend_rt_visible_devices.count(',')|default('0')|int %}{% set ns.cnt = ns.cnt + num + 1 %}{% endfor %}{{ offsets }}" #15.9、为 Decode 节点分配偏移量（用于多卡任务）
      run_once: yes
      delegate_to: localhost
      tags: 
        - run_server
        - run_proxy
        - always
    
    - name: Register values for prefill.
      set_fact:
        NODE_IP_LIST: >- #16.1、找出P节点中与当前主机host_ip相同的主机的ansible_ip，并生成列表
          {{
            groups['P'] | 
            map('extract', hostvars) | 
            selectattr('host_ip', '==', host_ip) | 
            map(attribute='ansible_host') | 
            unique | 
            join(',')
          }}
        NNODES: >- #16.2、生成上述主机数量
          {{
            groups['P'] | 
            map('extract', hostvars) | 
            selectattr('host_ip', '==', host_ip) | 
            list | 
            length 
          }}
      when: "'P' in group_names"
      tags: 
        - run_server
        - run_proxy
        - always

    - name: Display all values. #17、显示变量信息，用于调试
      debug:
        msg: |
         PREFILL_API_SERVER_LIST: {{ PREFILL_API_SERVER_LIST }}
         DECODE_API_SERVER_LIST: {{ DECODE_API_SERVER_LIST }}
         DECODE_SERVER_IP_LIST: {{ DECODE_SERVER_IP_LIST }}
         PREFILL_POD_NUM: {{ PREFILL_POD_NUM }}
         DECODE_NUM_DP: {{ DECODE_SERVER_ALL.count(',') + 1 }}
         DECODE_SERVER_OFFSET: {{ DECODE_SERVER_OFFSET }}
      run_once: yes
      delegate_to: localhost
      tags:
        - always

    - name: Generate a script to kill all Python processes in P container. #18、在P节点的SCRIPTS_PATH/kill_python_processes.sh路径上复制杀死python进程的脚本
      copy:
        content: "{{ kill_python_processes_cmd }}"
        dest: "$SCRIPTS_PATH/kill_python_processes.sh"
        mode: '0750'
      when: "'P' in group_names"
      tags: 
        - run_server
        - run_p

    - name: Generate a script to kill all Python processes in D container. #18、在D节点的SCRIPTS_PATH/kill_python_processes.sh路径上复制杀死python进程的脚本
      copy:
        content: "{{ kill_python_processes_cmd }}"
        dest: "$SCRIPTS_PATH/kill_python_processes.sh"
        mode: '0750'
      when: "'D' in group_names"
      tags: 
        - run_server
        - run_d

    - name: Kill all Python processes in the container of prefill. #19、在P节点的容器上执行杀死python进程的脚本
      command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_P /bin/bash -c $SCRIPTS_PATH/kill_python_processes.sh"
      environment:
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
      failed_when: false
      no_log: true
      when: "'P' in group_names"
      tags: 
        - run_server
        - run_p

    - name: Kill all Python processes in the container of decode. #20、在D节点的容器上执行杀死python进程的脚本
      command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_D /bin/bash -c $SCRIPTS_PATH/kill_python_processes.sh"
      environment:
        DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
      failed_when: false
      no_log: true
      when: "'D' in group_names"
      tags: 
        - run_server
        - run_d


    - name: Generate a script to run the sglang server for the prefill instances. #21、将run_sglang_server_prefill_cmd脚本的内容复制到P节点上的SCRIPTS_PATH/sglang_run_for_p.sh文件中
      copy:
        content: "{{ run_sglang_server_prefill_cmd }}"
        dest: "$SCRIPTS_PATH/sglang_run_for_p.sh"
        mode: '0750'
      when: "'P' in group_names"
      tags: 
        - run_server
        - run_p

    - name: Generate a script to run the sglang server for the decode instances. #22、将run_sglang_server_decode_cmd脚本的内容复制到D节点上的SCRIPTS_PATH/sglang_run_for_d.sh文件中
      copy:
        content: "{{ run_sglang_server_decode_cmd }}"
        dest: "$SCRIPTS_PATH/sglang_run_for_d.sh"
        mode: '0750'
      vars:
        server_offset_dict: "{{ DECODE_SERVER_OFFSET }}"
      when: "'D' in group_names"
      tags: 
        - run_server
        - run_d


    - name: Get socket name for communication between prefill instances and decode instances. #23、获取当前主机的默认网络接口，用于P、D节点间的通信
      shell: |
        ip -4 route list 0/0 | awk '{print $5}' | head -1
      register: default_interface_result
      changed_when: false
      tags: 
        - run_server
        - always

    - name: Use a variable to store the socket name. #24、将网络接口名称保存在变量中
      set_fact:
        default_interface: "{{ default_interface_result.stdout }}"
      when: default_interface_result.stdout != ""
      tags: 
        - run_server
        - always

    - name: Run the Omniai service for prefill instances. #25、当P节点数量不小于1时，执行docker_start_sglang_cmd_p脚本，在P节点容器内启动server服务
      command: bash -c "{{ docker_start_sglang_cmd_p }}"
      environment:
        ROLE: "P"
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
        PREFILL_SERVER_LIST: "{{ ascend_rt_visible_devices }}"
        API_PORT: "{{ api_port }}"
        SERVER_IP_LIST: "{{ DECODE_SERVER_IP_LIST | replace(' ', '') | trim }}"
        PREFILL_POD_NUM: "{{ PREFILL_POD_NUM }}"
        SOCKET_IFNAME: "{{ default_interface }}"  # TODO 验证是否可删除
        IP: "{{ ansible_host }}"
        HOST_IP: "{{ host_ip }}"
        NODE_RANK: "{{ node_rank }}"
        PREFILL_TENSOR_PARALLEL_SIZE: "{{ ascend_rt_visible_devices.split(',') | length }}"
        NNODES: "{{ NNODES }}"
        NODE_IP_LIST: "{{ NODE_IP_LIST }}"
      when: "'P' in group_names and (NODE_IP_LIST | string).split(',') | length >= 2"
      tags: 
        - run_server
        - run_p

    - name: Run the Omniai service for decode instances. #26、执行docker_start_sglang_cmd_d脚本，在D节点容器内启动server服务
      command: bash -c "{{ docker_start_sglang_cmd_d }}"
      environment:
        ROLE: "D"
        DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
        API_PORT: "{{ api_port }}"
        DECODE_SERVER_LIST: "{{ ascend_rt_visible_devices }}"
        SERVER_IP_LIST: "{{ DECODE_SERVER_IP_LIST | replace(' ', '') | trim }}"
        PREFILL_POD_NUM: "{{ PREFILL_POD_NUM }}"
        DECODE_POD_NUM: "{{ DECODE_POD_NUM }}"
        SOCKET_IFNAME: "{{ default_interface }}"   # TODO 验证是否可删除
        NUM_SERVERS: "{{ ascend_rt_visible_devices.split(',') | length }}"
        NODE_RANK: "{{ node_rank }}"
        IP: "{{ ansible_host }}"
        NNODES: "{{ nnodes }}"
        HOST_IP: "{{ host_ip }}"
        DECODE_DATA_PARALLEL_SIZE: "{{ DECODE_SERVER_ALL }}"
        HOST: "{{ inventory_hostname }}"
      when: "'D' in group_names"
      tags: 
        - run_server
        - run_d

    - name: Run the Omniai service for prefill instances. #27、当P节点数量为1时，执行docker_start_sglang_cmd_p脚本，在P节点容器内启动server服务
      command: bash -c "{{ docker_start_sglang_cmd_p }}"
      environment:
        ROLE: "P"
        DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
        PREFILL_SERVER_LIST: "{{ ascend_rt_visible_devices }}"
        API_PORT: "{{ api_port }}"
        SERVER_IP_LIST: "{{ DECODE_SERVER_IP_LIST | replace(' ', '') | trim }}"
        PREFILL_POD_NUM: "{{ PREFILL_POD_NUM }}"
        SOCKET_IFNAME: "{{ default_interface }}"
        IP: "{{ ansible_host }}"
        HOST_IP: "{{ host_ip }}"
        NODE_RANK: "{{ node_rank }}"
        PREFILL_TENSOR_PARALLEL_SIZE: "{{ ascend_rt_visible_devices.split(',') | length }}"
        NNODES: "{{ NNODES }}"
        NODE_IP_LIST: "{{ NODE_IP_LIST }}"
      when: "'P' in group_names and (NODE_IP_LIST | string).split(',') | length == 1"
      tags: 
        - run_server
        - run_p

    - name: Generate a script to kill all nginx processes in the container. #28、将杀死nginx进程的脚本复制到执行机的SCRIPTS_PATH/kill_nginx_processes.sh文件下
      copy:
        content: "{{ kill_nginx_processes_cmd }}"
        dest: "$SCRIPTS_PATH/kill_nginx_processes.sh"
        mode: '0750'
      when: "'C' in group_names"
      tags: 
        - run_proxy
        - run_c

    - name: Kill all Python processes in the container of prefill. #29、在执行机的容器中执行杀死nginx进程的脚本
      command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_C /bin/bash -c $SCRIPTS_PATH/kill_nginx_processes.sh"
      environment:
        DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
      failed_when: false
      no_log: true
      when: "'C' in group_names"
      tags: 
        - run_proxy
        - run_c

    - name: Generate a script to run the global proxy server. #30、将run_proxy_cmd的内容复制到C的SCRIPTS_PATH/run_proxy_server.sh的文件中
      copy:
        content: "{{ run_proxy_cmd }}"
        dest: "$SCRIPTS_PATH/run_proxy_server.sh"
        mode: '0750'
      when: "'C' in group_names"
      tags: 
        - run_proxy
        - run_c

    - name: Run the global proxy server. #31、在C的容器中执行开启proxy服务的脚本
      command: bash -c "{{ docker_start_proxy_cmd_c }}"
      environment:
        DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
      when: "'C' in group_names"
      tags: 
        - run_proxy
        - run_c

    - name: Create a directory on the executor to store the log. #32、在执行机上创建目录以存放日志文件
      ansible.builtin.file:
        path: "{{ ansible_env.LOG_PATH_IN_EXECUTOR }}/{{ inventory_hostname }}"
        state: directory
      when: "'P' in group_names or 'D' in group_names"
      delegate_to: localhost
      tags: 
        - fetch_log

    - name: Forward logs from all machines to the executor. #33、将P、D节点上的日志文件复制到执行机上
      ansible.builtin.synchronize:
        mode: pull
        src: "{{ ansible_env.LOG_PATH }}"
        dest: "{{ ansible_env.LOG_PATH_IN_EXECUTOR }}/{{ inventory_hostname }}/"
      when: "'P' in group_names or 'D' in group_names"
      delegate_to: localhost
      tags: 
        - fetch_log
